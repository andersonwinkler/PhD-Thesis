\chapter{Permutation inference}
\label{sec:permglm}
\setstretch{\lspac}

The field of neuroimaging has continuously expanded to encompass an ever growing variety of experimental methods. From the early experiments using positron emission tomography (\textsc{pet}) and functional magnetic resonance imaging (\textsc{fmri}), it is now often of interest to verify hypotheses using information obtained from, e.g.\ tensor-based morphometry (\textsc{tbm}), diffusion tensor imaging (\textsc{dti}), cortical thickness and surface area, cerebral perfusion, as well as many others and variations and combinations of these. All these different modalities produce images that have different physical and biological properties, as well as different information content. Despite this variety, most of the strategies for statistical analysis constitute linear models, and can be formulated within the \emph{general linear model} (\textsc{glm}). The \textsc{glm} is a simple, yet flexible framework of which many different types of analysis are particular cases \citep{Scheffe1959, Searle1971, Christensen2002}. The common strategy is to construct a plausible explanatory model for the observed data, estimate the parameters of this model, and compute a suitable statistic for testing of hypotheses on some or all of these parameters. The rejection or acceptance of a given hypothesis depends on the probablility of finding, due to chance alone, a statistic at least as high as the observed. Typically, but not necessarily, the hypothesis being tested is that one or more parameters are zero, being referred to as the \emph{null hypothesis}.

If the parameters of the distribution of the statistic under the hypothesis being tested, be it the null or not, is known, such probability can be ascertained using this distribution. In many particular cases, a mathematical expression describes the behaviour of the statistic as a function of these parameters, and this analytical representation of the distribution can be used for hypotheses testing as long as the data satisfies a certain set of requirements under which the distribution arises and can be recovered asymptotically. A conclusion based on these \emph{parametric tests} will only be sound as long as the observed data possess these assumed stochastic properties, even if other methodological aspects are valid. Strategies that may be used when these assumptions are not met include, among others, the use of \emph{non-parametric tests}.

\emph{Permutation tests} are a class of non-parametric methods. They were pioneered by \citet{Fisher1935} and \citet{Pitman1937-I, Pitman1937-II, Pitman1938}. Fisher demonstrated that the null hypothesis could be tested simply by observing, after permuting observations, how often the difference between means would exceed the difference found without permutation, and that for such test, no normality would be required. Pitman provided the first complete mathematical framework for permutation methods, although similar ideas, based on actually repeating an experiment many times with the experimental conditions being permuted, can be found even earlier \citep{Peirce1884}. Important theoretical and practical advances have been ongoing in the past decades \citep{Pearson1937, Scheffe1943, Lehmann1949, Kempthorne1955, Edgington1995, Good2002, Good2005, Westfall2008, Pesarin2010}, and usage only became practical after the availability sufficient computing power \citep{Efron1979}.

In neuroimaging, permutation methods were first proposed by \citet{Blair1994} for electroencephalography, and later by \citet{Holmes1996} for positron-emission tomography, with the objective of allowing inferences while taking into account the multiplicity of tests. These early permutation approaches already accounted for the spatial smoothness of the image data. \citet{Arndt1996} proposed a permutation scheme for testing the omnibus hypothesis of whether two sets of images would differ. Structural magnetic resonance imaging (\textsc{mri}) data were considered by \citet{Bullmore1999}, who developed methods for omnibus, voxel and cluster-mass inference, controlling the expected number of false positives.

Single subject experiments from functional magnetic resonance imaging (\textsc{fmri}) presents a challenge to permutation methods, as serial autocorrelation in the time series violates the fundamental assumption needed for permutation, that of exchangeability (discussed below). Even though some early work did not fully account for autocorrelation \citep{Belmonte2001}, other methods that accommodated the temporally correlated nature of the \textsc{fmri} signal and noise were developed \citep{Bullmore1996, Bullmore2001, Locascio1997, Brammer1997, Breakspear2004, Laird2004}. Some of these methods use a single reference distribution constructed by pooling permutation statistics over space from a small number of random permutations, under the (untenable and often invalid) assumption of spatial homogeneity of distributions.

\citet{Nichols2002} provided a practical description of permutation methods for \textsc{pet} and multi-subject \textsc{fmri} studies, but noted the challenges posed by nuisance variables. Permutation inference is grounded on \emph{exchangeability} under the null hypothesis, that data can be permuted (exchanged) without affecting its joint distribution. However, if a nuisance effect is present in the model, the data cannot be considered exchangeable even under the null hypothesis.  For example, if one wanted to test for sex differences while controlling for the linear effect of age, the null hypothesis is ``male mean equals female mean'', while allowing age differences; the problem is that, even when there is no sex effect, a possible age effect may be present, e.g., younger and older individuals being different, then the data are not directly exchangeable under this null hypothesis. Another case where this arises is in factorial experiments, where one factor is to be tested in the presence of another, or where their interaction is to be tested in the presence of main effects of either or both. Although permutation strategies for factorial experiments in neuroimaging were considered by \citet{Suckling2004}, a more complete general framework to account for nuisance variables is still missing.

In this paper we review the statistical literature for the \textsc{glm} with arbitrary designs and contrasts, emphasizing useful aspects, yet that have not been considered for neuroimaging, unify this diverse set of results into a single permutation strategy and a single generalised statistic, present implementation strategies for efficient computation and provide a complete algorithm, conduct detailed simulations and evaluations in various settings, and identify certain methods that generally outperforms others. We will not consider intrasubject (timeseries) \textsc{fmri} data, focusing instead on modelling data with independent observations or sets of repeated observations from independent subjects. We give examples of applications to common designs and discuss how these methods, originally intended for independent data, can in special cases be extended to repeated measurements and longitudinal designs.

\section{Theory}

\subsection{Model and notation}
\label{sec:model}

At each spatial point (voxel, vertex or face) of an image representation of the brain, a general linear model \citep{Searle1971} can be formulated and expressed as:

\begin{equation}
\mathbf{Y} =  \mathbf{M} \boldsymbol{\psi} + \boldsymbol{\epsilon}
\label{eqn:glmfull}
\end{equation}

\noindent
where $\mathbf{Y}$ is the $N \times 1$ vector of observed data\footnote{While we focus on univariate data, the general principles presented can be applied to multivariate linear models.}, $\mathbf{M}$ is the full-rank $N \times r$ design matrix that includes all effects of interest as well as all modelled nuisance effects, $\boldsymbol{\psi}$ is the $r \times 1$ vector of $r$ regression coefficients, and $\boldsymbol{\epsilon}$ is the $N \times 1$ vector of random errors. In permutation tests, the errors are not assumed to follow a normal distribution, although some distributional assumptions are needed, as detailed below. Estimates for the regression coefficients can be computed as $\boldsymbol{\hat{\psi}} = \mathbf{M}^{+}\mathbf{Y}$, where the superscript ($^{+}$) denotes the Moore--Penrose pseudo-inverse. Our interest is to test the null hypothesis that an arbitrary combination (contrast) of some or all of these parameters is equal to zero, i.e., $\mathcal{H}_{0} : \mathbf{C}'\boldsymbol{\psi} =\boldsymbol{0}$, where $\mathbf{C}$ is a $r \times s$ full-rank matrix of $s$ contrasts, $1 \leqslant s \leqslant r$.

For the discussion that follows, it is useful to consider a transformation of the model in Equation~\ref{eqn:glmfull} into a partitioned one:

\begin{equation}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}
\label{eqn:glmpart}
\end{equation}

\noindent
where $\mathbf{X}$ is the matrix with regressors of interest, $\mathbf{Z}$ is the matrix with nuisance regressors, and $\boldsymbol{\beta}$ and $\boldsymbol{\gamma}$ are respectively the vectors of regression coefficients. Even though such partitioning is not unique, it can be defined in terms of the contrast $\mathbf{C}$ in a way that inference on $\boldsymbol{\beta}$ is equivalent to inference on $\mathbf{C}'\boldsymbol{\psi}$, as described in \ref{sec:partitioning}.

As the models expressed in Equations \ref{eqn:glmfull} and \ref{eqn:glmpart} are equivalent, their residuals are the same and can be obtained as  $\boldsymbol{\hat{\epsilon}} = \mathbf{R}_{\mathbf{M}}\mathbf{Y}$, where $\mathbf{R}_{\mathbf{M}}=\mathbf{I}-\mathbf{H}_{\mathbf{M}}$ is the residual-forming matrix, $\mathbf{H}_{\mathbf{M}}=\mathbf{M}\mathbf{M}^{+}$ is the projection (``hat'') matrix, and $\mathbf{I}$ is the $N \times N$ identity matrix. The residuals due to the nuisance alone are $\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}} = \mathbf{R}_{\mathbf{Z}}\mathbf{Y}$, where $\mathbf{R}_{\mathbf{Z}}=\mathbf{I}-\mathbf{H}_{\mathbf{Z}}$, and $\mathbf{H}_{\mathbf{Z}} = \mathbf{Z}\mathbf{Z}^{+}$. For permutation methods, an important detail of the linear model is the non-independence of residuals, even when errors $\boldsymbol{\epsilon}$ are independent and have constant variance, a fact that contributes to render these methods approximately exact. For example, in that setting $\mathsf{E}\left(\mathsf{Var}(\boldsymbol{\hat{\epsilon}_{\mathbf{Z}}})\right)=\mathbf{R}_{\mathbf{Z}}\neq\mathbf{I}$. The commonly used $F$ statistic can be computed as \citep{Christensen2002}:

\begin{equation}
\begin{array}{ccl}
F &=& \left.\dfrac{\boldsymbol{\hat{\psi}}'\mathbf{C} \left(\mathbf{C}'(\mathbf{M}'\mathbf{M})^{-1}\mathbf{C} \right)^{-1} \mathbf{C}'\boldsymbol{\hat{\psi}}}{\mathsf{rank}\left(\mathbf{C}\right)} \right/ \dfrac{\boldsymbol{\hat{\epsilon}}'\boldsymbol{\hat{\epsilon}}}{N-\mathsf{rank}\left(\mathbf{M}\right)}\\[15pt]
{} & = & \left.\dfrac{\boldsymbol{\hat{\beta}}' \left(\mathbf{X}'\mathbf{X}\right)\boldsymbol{\hat{\beta}}}{\mathsf{rank}\left(\mathbf{C}\right)} \right/ \dfrac{\boldsymbol{\hat{\epsilon}}'\boldsymbol{\hat{\epsilon}}}{n-\mathsf{rank}\left(\mathbf{X}\right)-\mathsf{rank}\left(\mathbf{Z}\right)}
\end{array}
\label{eqn:fstat}
\end{equation}

\noindent
When $\mathsf{rank}(\mathbf{C}) = 1$, $\boldsymbol{\hat{\beta}}$ is a scalar and the Student's $t$ statistic can be expressed as a function of $F$ as $t=\mathsf{sign}(\boldsymbol{\hat{\beta}})\sqrt{F}$.

\paragraph{Choice of the statistic} In non-parametric settings we are not constrained to the $F$ or $t$ statistics and, in principle, any statistic where large values reflect evidence against the null hypothesis could be used. This includes regression coefficients or descriptive statistics, such as differences between medians, trimmed means or ranks of observations \citep{Ernst2004}. However, the statistic should be chosen such that it does not depend on the scale of measurement or on any unknown parameter. The regression coefficients, for instance, whose variance depends both on the error variance and on the colinearity of that regressor with the others, are not in practice a good choice, as certain permutation schemes alter the colinearity among regressors \citep{Kennedy1996}. Specifically with respect to brain imaging, the correction for multiple testing (discussed later) requires that the statistic has a distribution that is spatially homogeneous, something that regression coefficients cannot provide. In parametric settings, statistics that are independent of any unknown parameters are called \emph{pivotal statistics}. Statistics that are pivotal or asymptotically pivotal are appropriate and facilitate the equivalence of the tests across the brain, and their advantages are well established for related non-parametric methods \citep{Hall1991, Westfall1993}. Examples of such pivotal statistics include the Student's $t$, the $F$ ratio, the Pearson's correlation coefficient (often known as $r$), the coefficient of determination ($R^2$), as well as most other statistics used to construct confidence intervals and to compute p-values in parametric tests. We will return to the matter of pivotality when discussing exchangeability blocks, and the choice of an appropriate statistic for these cases.

\paragraph{p-values} Regardless of the choice of the test statistic, p-values offer a common measure of evidence against the null hypothesis. For a certain test statistic $T$, which can be any of those discussed above, and a particular observed value $T_{0}$ of this statistic after the experiment has been conducted, the p-value is the probability of observing, by chance, a test statistic equal or larger than the one computed with the observed values, i.e., $\text{p-value}=P(T \geqslant T_{0} | \mathcal{H}_{0})$. Although here we only consider one-sided tests, where evidence against $\mathcal{H}_{0}$ corresponds to larger values of $T_{0}$, two-sided or negative-valued tests and their p-values can be similarly defined. In parametric settings, under a number of assumptions, the p-values can be obtained by referring to the theoretical distribution of the chosen statistic (such as the $F$ distribution), either through a known formula, or using tabulated values. In non-parametric settings, these assumptions are avoided. Instead, the data are randomly shuffled, many times, in a manner consistent with the null hypothesis. The model is fitted repeatedly once for every shuffle, and for each fit a new realisation of the statistic, $T_{j}^{*}$, is computed, being $j$ a permutation index. An empirical distribution of $T^{*}$ under the null hypothesis is constructed, and from this null distribution a p-value is computed as $\frac{1}{J}\sum_{j}I(T^{*}_j \geqslant T_{0})$, where $J$ is the number of shufflings performed, and $I(\cdot)$ is the indicator function. From this it can be seen that the non-parametric p-values are discrete, with each possible p-value being a multiple of $1/J$. It is important to note that the permutation distribution should include the observed statistic without permutation \citep{Edgington1969, Phipson2010}, and thus the smallest possible p-value is $1/J$, not zero.

\subsection{Model partitioning}
\label{sec:partitioning}

The permutation methods discussed in this paper require that the design matrix $\mathbf{M}$ is partitioned into effects of interest and nuisance effects. Such partitioning is not unique, and schemes can be as simple as separating apart the columns of $\mathbf{M}$ as $\left[ \mathbf{X} \; \mathbf{Z} \right]$, with $\boldsymbol{\psi} = \left[ \boldsymbol{\beta}' \; \boldsymbol{\gamma}' \right]'$ \citep{Guttman1982}. More involved strategies can, however, be devised to obtain some practical benefits. One such partitioning is to define $\mathbf{X} = \mathbf{M} \mathbf{D} \mathbf{C} \left(\mathbf{C}'\mathbf{D}\mathbf{C}\right)^{-1}$ and 
$\mathbf{Z} = \mathbf{M} \mathbf{D} \mathbf{C}_v \left(\mathbf{C}_v'\mathbf{D}\mathbf{C}_v\right)^{-1}$, where $\mathbf{D}=(\mathbf{M}'\mathbf{M})^{-1}$, $\mathbf{C}_v=\mathbf{C}_u-\mathbf{C}(\mathbf{C}'\mathbf{D}\mathbf{C})^{-1}\mathbf{C}'\mathbf{D}\mathbf{C}_u$, and $\mathbf{C}_u$ has $r-\mathsf{rank}\left(\mathbf{C}\right)$ columns that span the null space of $\mathbf{C}$, such that $[\mathbf{C} \; \mathbf{C}_u]$ is a $r \times r$ invertible, full-rank matrix \citep{Beckmann2001, Smith2007}. This partitioning has a number of features: $\boldsymbol{\hat{\beta}} = \mathbf{C}'\boldsymbol{\hat{\psi}}$, $\widehat{\mathsf{Cov}}(\boldsymbol{\hat{\beta}}) = \mathbf{C}'\widehat{\mathsf{Cov}}(\boldsymbol{\hat{\psi}})\mathbf{C}$, i.e., estimates and variances of $\boldsymbol{\beta}$ for inference on the partitioned model correspond exactly to the same inference on the original model, $\mathbf{X}$ is orthogonal to $\mathbf{Z}$, and $\mathsf{span}(\mathbf{X}) \cup \mathsf{span}(\mathbf{Z}) = \mathsf{span}(\mathbf{M})$, i.e.,\ the partitioned model spans the same space as the original. This is the partitioning strategy used in this paper, and used in the \texttt{randomise} algorithm.

Another useful partitioning scheme, derived by \citet{Ridgway2009}, defines $\mathbf{X}=\mathbf{M}(\mathbf{C}^+)'$ and $\mathbf{Z}=\mathbf{M}-\mathbf{M}\mathbf{C}\mathbf{C}^{+}$. As with the previous strategy, the parameters of interest in the partitioned model are equal to the contrast of the original parameters. A full column rank nuisance partition can be obtained from the singular value decomposition (\textsc{svd}) of $\mathbf{Z}$, which will also provide orthonormal columns for the nuisance partition. Orthogonality between regressors of interest and nuisance can be obtained by redefining the regressors of interest as  $\mathbf{R}_{\mathbf{Z}}\mathbf{X}$.

\subsection{Permutations and exchangeability}

Perhaps the most important aspect of permutation tests is the manner in which data are shuffled under the null hypothesis. It is the null hypothesis, together with assumptions about exchangeability, that determines the permutation strategy. Let the $j$-th permutation be expressed by $\mathbf{P}_{j}$, a $N \times N$ permutation matrix, a matrix that has all elements being either $0$ or $1$, each row and column having exactly one $1$ (Figure~\ref{fig:pmatrices}a). Pre-multiplication of a matrix by $\mathbf{P}_{j}$ permutes its rows. We denote $\mathcal{P}=\{\mathbf{P}_{j}\}$ the set of all permutation matrices under consideration, indexed by the subscript $j$. We similarly define a sign flipping matrix $\mathbf{S}_{j}$, a $N \times N$ diagonal matrix whose non-zero elements consist only of $+1$ or $-1$ (Figure~\ref{fig:pmatrices}b). Pre-multiplication of a matrix by $\mathbf{S}_{j}$ implements a set of sign flips for each row. Likewise, $\mathcal{S}=\{\mathbf{S}_{j}\}$ denotes the set of all sign flipping matrices under consideration. We consider also both schemes together, where $\mathbf{B}_{j}=\mathbf{P}_{j'}\mathbf{S}_{j''}$ implements sign flips followed by permutation; the set of all possible such transformations is denoted as $\mathcal{B}=\{\mathbf{B}_{j}\}$. Throughout the paper, we use generic terms as \emph{shuffling} or \emph{rearrangement} whenever the distinction between permutation, sign flipping or combined permutation with sign flipping is not pertinent. Finally, let $\boldsymbol{\hat{\beta}}^{*}_{j}$ and $T^{*}_{j}$, respectively, be the estimated regression coefficients and the computed statistic for the shuffling $j$. 

\begin{figure}[!p]
\centering
\includegraphics{images/pmatrices.eps}
\caption[Examples of permutation and sign flipping matrix.]{Examples of a permutation matrix (a), of a sign flipping matrix (b), and of a matrix that does permutation and sign flipping (c). Pre-multiplication by a permutation matrix shuffles the order of the data, whereas by a sign flipping matrix changes the sign of a random subset of datapoints.}
\label{fig:pmatrices}
\end{figure}

The essential assumption of permutation methods is that, for a given set of variables, \emph{their joint probability distribution does not change if the observations are rearranged}. This can be expressed in terms of exchangeable errors or independent and symmetric errors, each of these weakening different assumptions when compared to parametric methods.

\emph{Exchangeable errors} (\textsc{ee}) is the traditional permutation requirement \citep{Good2005}. The formal statement is that, for any permutation $\mathbf{P}_{j} \in \mathcal{P}$,

\begin{equation}
\boldsymbol{\epsilon} \stackrel{\mathsf{d}}{=} \mathbf{P}_{j}\boldsymbol{\epsilon}
\end{equation}

\noindent where the symbol $\stackrel{\mathsf{d}}{=}$ denotes equality of distributions. In other words, the errors are considered exchangeable if their joint distribution is invariant with respect to permutation. Exchangeability is similar to, yet more general than, independence, as exchangeable errors can have all-equal and homogeneous dependence. Relative to the common parametric assumptions of independent, normally and identically distributed (iid) errors, \textsc{ee} relaxes two aspects. First, normality is no longer assumed, although identical distributions are required. Second, the independence assumption is weakened slightly to allow exchangeability when the observations are not independent, but their joint distribution is maintained after permutation. While exchangeability is a general condition that applies to any distribution, the multivariate normal distribution is indeed exchangeable if the marginal distributions are uncorrelated, or if the off-diagonal elements of the covariance matrix are identical to each other (not necessarily equal to zero).\footnote{In parametric settings, such dependence structure is often referred to as \emph{compound symmetry}.}

\emph{Independent and symmetric errors} (\textsc{ise}) can be considered for measurements that arise, for instance, from differences between two groups if the variances are not assumed to be the same. The formal statement for permutation under \textsc{ise} is that for any sign flipping matrix $\mathbf{S}_{j} \in \mathcal{S}$,

\begin{equation}
\boldsymbol{\epsilon} \stackrel{\mathsf{d}}{=} \mathbf{S}_{j}\boldsymbol{\epsilon}
\end{equation}

\noindent
that is, the joint distribution of the error terms is invariant with respect to sign flipping. Relative to the parametric assumptions of independent, normally and identically distributed errors, \textsc{ise} relaxes normality, although symmetry of distributions is required. Independence is also required to allow sign flipping of one observation without perturbing others.

Although the \textsc{ee} does not require symmetry for the distribution of the error terms, it requires that the variances and covariances of the error terms are all equal, or have a structure that is compatible with the definition of exchangebility blocks (discussed below). While the \textsc{ise} assumption has yet more stringent requirements, if both \textsc{ee} and \textsc{ise} are plausible and available for a given model, permutations and sign flippings can be performed together, increasing the number of possible rearrangements, a feature particularly useful for studies with small sample sizes. The formal statement for shuffling under both \textsc{ee} and \textsc{ise} is that, as with the previous cases, for any matrix $\mathbf{B}_{j} \in \mathcal{B}$,

\begin{equation}
\boldsymbol{\epsilon} \stackrel{\mathsf{d}}{=} \mathbf{B}_{j}\boldsymbol{\epsilon}
\end{equation}

\noindent
that is, the joint distribution of the error terms remains unchanged under both permutation and sign flipping.

There are yet other important aspects related to exchangeability. The experimental design may dictate blocks of observations that are jointly exchangeable, allowing data to be permuted within block or, alternatively, that the blocks may themselves be exchangeable as a whole. This is the case, for instance, for designs that involve multiple observations from each subject. While permutation methods generally do not easily deal with non-independent data, the definition of these \emph{exchangeability blocks} (\textsc{eb}) allows these special cases of well structured dependence to be accommodated. While \textsc{eb} determine how the data shufflings are performed, they should not be confused with \emph{variance groups} (\textsc{vg}), i.e., groups of observations that are known or assumed to have similar variances, which can be pooled for estimation and computation of the statistic. Variance groups need to be compatible with, yet not necessarily identical to, the exchangeability blocks, as discussed in Section~\ref{sec:restricted}. A summary of the properties discussed this far and some benefits of permutation methods are shown in Table~\ref{tab:assumptions}.

\begin{table}[!p]
\caption[Summary of assumptions of permutation methods.]{Compared with parametric methods, permutation tests relax a number of assumptions and can be used in a wider variety of situations. Some of these assumptions can be further relaxed with the definition of exchangeability blocks.}
\begin{center}
{\small
\begin{tabular}{@{}l@{}m{17mm}<{\centering}m{13mm}<{\centering}m{17mm}<{\centering}@{}}
\toprule
Assumptions                                & \textsc{ee} & \textsc{ise} & Parametric\\
\midrule
\multicolumn{4}{l}{\emph{With respect to the dependence structure between error terms:}}\\
Independent                                & {\color{blue}\ding{'63}}  & {\color{blue}\ding{'63}}  & {\color{blue}\ding{'63}}\\
Non-independent, exchangeable              & {\color{blue}\ding{'63}}  & {\color{red}\ding{'67}}   & {\color{red}\ding{'67}}\\
Non-independent, non-exchangeable          & {\color{red}\ding{'67}}   & {\color{red}\ding{'67}}   & {\color{red}\ding{'67}}\\
\midrule
\multicolumn{4}{l}{\emph{With respect to the distributions of the error terms:}}\\
Normal, identical                          & {\color{blue}\ding{'63}}  & {\color{blue}\ding{'63}}  & {\color{blue}\ding{'63}}\\
Symmetrical, identical                     & {\color{blue}\ding{'63}}  & {\color{blue}\ding{'63}}  & {\color{red}\ding{'67}}\\
Symmetrical, non-identical                 & {\color{red}\ding{'67}}   & {\color{blue}\ding{'63}}  & {\color{red}\ding{'67}}\\
Skewed, identical                          & {\color{blue}\ding{'63}}  & {\color{red}\ding{'67}}   & {\color{red}\ding{'67}}\\
Skewed, non-identical                      & {\color{red}\ding{'67}}   & {\color{red}\ding{'67}}   & {\color{red}\ding{'67}}\\
\bottomrule
\end{tabular}}
{\footnotesize
\begin{enumerate}
\item[{\color{blue}\ding{'63}}] Can be used directly if the assumptions regarding dependence structure and \newline distribution of the error terms are both met.
\item[{\color{red}\ding{'67}}] Cannot be used directly, or can be used in particular cases.
\end{enumerate}\par}
\end{center}
\label{tab:assumptions}
\end{table}

\subsubsection{Unrestricted exchangeability}
\label{sec:unrestricted}

In the absence of nuisance variables, the model reduces to $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, and under the null hypothesis $\mathcal{H}_0:\boldsymbol{\beta}=\boldsymbol{0}$ the data are pure error, $\mathbf{Y}=\boldsymbol{\epsilon}$.  Thus the \textsc{ee} or \textsc{ise} assumptions on the \emph{error} (presented above) justify freely permuting or sign flipping the \emph{data} under $\mathcal{H}_0$.  It is equivalent, however, to alter the design instead of the data. For example, for a nuisance-free design,

\begin{equation}
\mathbf{P}\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\;\;\Longleftrightarrow\;\;
\mathbf{Y}=\mathbf{P}'\mathbf{X}\boldsymbol{\beta}+\mathbf{P}'\boldsymbol{\epsilon}
\end{equation}

\noindent
since permutation matrices $\mathbf{P}$ are orthogonal; the same holds for sign flipping matrices $\mathbf{S}$. This is an important computational consideration as altering the design is much less burdensome than altering the image data. Also note that the errors $\boldsymbol{\epsilon}$ are not observed and thus never directly altered; going forward we will suppress any notation indicating permutation or sign flipping of the errors.

In the presence of nuisance variables (Equation~\ref{eqn:glmpart}), however, the problem is more complex. If the nuisance coefficients $\boldsymbol{\gamma}$ were somehow known, an exact permutation test would be available:

\begin{equation}
\mathbf{Y} - \mathbf{Z}\boldsymbol{\gamma} = \mathbf{P}\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\end{equation}

The perfectly adjusted data $\mathbf{Y} - \mathbf{Z}\boldsymbol{\gamma}$ are then pure error under $\mathcal{H}_0$ and inference could proceed as above. In practice, the nuisance cofficients have to be estimated and the adjusted data will not behave as $\boldsymbol{\epsilon}$.  For example, the obvious solution is to use the nuisance-only residuals $\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}$ as the adjusted data.  However, as noted above, residuals induce dependence and any \textsc{ee} or \textsc{ise} assumptions on $\boldsymbol{\epsilon}$ will not be conveyed to $\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}$.

A number of approaches have been proposed to produce approximate p-values in these cases \citep{Draper1966, Beaton1978, Still1981, Brown1982, Levin1983, Freedman1983, Oja1987, Gail1988, Welch1990, TerBraak1992, Kennedy1995, Edgington1995, Manly1997, Huh2001, Jung2006, Kherad2010}. We present these methods in a common notation with detailed annotation in in Table~\ref{tab:methods}.  While a number of authors have made comparisons between some of these methods \citep{Kennedy1995, Kennedy1996, Gonzalez1998, Anderson1999, Anderson2001, Anderson2003, OGorman2005, Dekker2007, Nichols2008, Ridgway2009}, they often only approached particular cases, did not consider repeated measurements, did not use full matrix notation as more common in neuroimaging literature, and often did not consider implementation complexities due to the large size of imaging datasets. In this section we focus on the Freedman--Lane and the Smith methods, which, as we show in Section~\ref{sec:method_perm}, produce the best results in terms of control over error rates and power.

\begin{table}[p]
\caption[Methods available to construct the null distribution in the presence of nuisance variables.]{A number of methods are available to obtain parameter estimates and construct a reference distribution in the presence of nuisance variables.}
\begin{center}
{\small
\begin{tabular}{@{}m{3.9cm}r@{\hspace{1.8mm}}c@{\hspace{2.2mm}}l}
\toprule
Method &
\multicolumn{3}{c}{Model\hspace*{18mm}}\\
\midrule
%Exact$^{(a)}$ &
%$\mathbf{Y} - \mathbf{Z}\boldsymbol{\gamma}$ &$=$& $\mathbf{P}\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$ \\
Draper--Stoneman$^{(a)}$ & 
$\mathbf{Y}$ &$=$& $\mathbf{P}\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}$ \\
Still--White$^{(b)}$ &
$\mathbf{P}\mathbf{R}_{\mathbf{Z}}\mathbf{Y}$ &$=$& $\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$  \\
Freedman--Lane$^{(c)}$ &
$\left(\mathbf{P}\mathbf{R}_{\mathbf{Z}}+\mathbf{H}_{\mathbf{Z}}\right)\mathbf{Y}$ &$=$& $\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon}$ \\
ter Braak$^{(d)}$ &
$\left(\mathbf{P}\mathbf{R}_{\mathbf{M}}+\mathbf{H}_{\mathbf{M}}\right)\mathbf{Y}$ &$=$& $\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon}$ \\
Kennedy$^{(e)}$ &
$\mathbf{P}\mathbf{R}_{\mathbf{Z}}\mathbf{Y}$ &$=$& $\mathbf{R}_{\mathbf{Z}}\mathbf{X}\boldsymbol{\beta} +  \boldsymbol{\epsilon}$ \\
Manly$^{(f)}$ &
$\mathbf{P}\mathbf{Y}$ &$=$& $\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}$\\
Huh--Jhun$^{(g)}$ &
$\mathbf{P}\mathbf{Q}'\mathbf{R}_{\mathbf{Z}}\mathbf{Y}$ &$=$& $\mathbf{Q}'\mathbf{R}_{\mathbf{Z}}\mathbf{X}\boldsymbol{\beta} +  \boldsymbol{\epsilon}$\\
Smith$^{(h)}$ &
$\mathbf{Y}$ &$=$& $\mathbf{P}\mathbf{R}_{\mathbf{Z}}\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}$ \\
Parametric$^{(i)}$ &
$\mathbf{Y}$ &$=$& $\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon},\;\; \boldsymbol{\epsilon}\sim\mathcal{N}(0,\sigma^2\mathbf{I})$ \\
\bottomrule
\end{tabular}}
\end{center}
{\footnotesize
%$(a)$ In the Exact method, the true $\boldsymbol{\gamma}$ is known and does not need to be estimated. Since it is known, $\mathbf{Z}\boldsymbol{\gamma}$ can be subtracted from $\mathbf{Y}$, and only $\boldsymbol{\beta}$ needs to be estimated. Permutation can be performed as for simple regression. The Exact method is used for comparison with the others in the simulations presented in Section~\ref{sec:method_perm}.
$(a)$ \citet{Draper1966}. This method was called ``Shuffle~Z'' by \citet{Kennedy1995}, and using the same notation adopted here, it would be called ``Shuffle~X''.
$(b)$ \citet{Still1981, Levin1983, Gail1988}.
$(c)$ \citet{Freedman1983}.
$(d)$ \citet{TerBraak1992}. The null distribution for this method considers $\boldsymbol{\hat{\beta}}^{*}_{j} = \boldsymbol{\hat{\beta}}$, i.e., the permutation happens under the alternative hypothesis, rather than the null.
$(e)$ \citet{Kennedy1995, Kennedy1996}. This method was referred to as ``Residualize both Y and Z'' in the original publication, and using the same notation adopted here, it would be called ``Residualize both Y and X''.
$(f)$ \citet{Manly1997}.
$(g)$ \citet{Huh2001, Jung2006, Kherad2010}. $\mathbf{Q}$ is a $N \times N'$ matrix, where $N'$ is the rank of $\mathbf{R}_{\mathbf{Z}}$. $\mathbf{Q}$ is computed through Schur decomposition of $\mathbf{R}_{\mathbf{Z}}$, such that $\mathbf{R}_{\mathbf{Z}}=\mathbf{Q}\mathbf{Q}'$ and $\mathbf{I}_{N' \times N'}=\mathbf{Q}'\mathbf{Q}$. For this method, $\mathbf{P}$ is $N' \times N'$. From the methods in the table, this is the only that cannot be used directly under restricted exchangeability, as the block structure is not preserved.
$(h)$ The Smith method consists of orthogonalization of $\mathbf{X}$ with respect to $\mathbf{Z}$. In the permutation and multiple regression literature, this method was suggested by a referee of \citet{OGorman2005}, and later presented by \citet{Nichols2008} and discussed by \citet{Ridgway2009}.
$(i)$ The parametric method does not use permutations, being instead based on distributional assumptions.\
$\square$ For all the methods, the left side of the equations contains the data (regressand), the right side the regressors and error terms. The unpermuted models can be obtained by replacing $\mathbf{P}$ for $\mathbf{I}$. Even for the unpermuted models, and even if $\mathbf{X}$ and $\mathbf{Z}$ are orthogonal, not all these methods produce the same error terms $\boldsymbol{\epsilon}$. This is the case, for instance, of the Kennedy and Huh--Jhun methods. Under orthogonality between $\mathbf{X}$ and $\mathbf{Z}$, some regression methods are equivalent to each other.}
\label{tab:methods}
\end{table}

The \emph{Freedman--Lane procedure} \citep{Freedman1983} can be performed through the following steps:

\begin{enumerate}
\item Regress $\mathbf{Y}$ against the full model that contains both the effects of interest and the nuisance variables, i.e.\ $\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}$. Use the estimated parameters $\boldsymbol{\hat{\beta}}$ to compute the statistic of interest, and call this statistic $T_{0}$.
\item Regress $\mathbf{Y}$ against a reduced model that contains only the nuisance effects, i.e.\ $\mathbf{Y} = \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}_{\mathbf{Z}}$, obtaining estimated parameters $\boldsymbol{\hat{\gamma}}$ and estimated residuals $\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}$.
\item Compute a set of permuted data $\mathbf{Y}^{*}_{j}$. This is done by pre-multiplying the residuals from the reduced model produced in the previous step, $\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}$, by a permutation matrix, $\mathbf{P}_{j}$, then adding back the estimated nuisance effects, i.e.\ $\mathbf{Y}^{*}_{j} = \mathbf{P}_{j}\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}} + \mathbf{Z}\boldsymbol{\hat{\gamma}}$. 
\item Regress the permuted data $\mathbf{Y}^{*}_{j}$ against the full model, i.e.\ $\mathbf{Y}^{*}_{j} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\epsilon}$, and use the estimated $\boldsymbol{\hat{\beta}}^{*}_{j}$ to compute the statistic of interest. Call this statistic $T^{*}_{j}$.
\item Repeat the Steps 2--4 many times to build the reference distribution of $T^{*}$ under the null hypothesis.
\item Count how many times $T^{*}_{j}$ was found to be equal or larger than $T_{0}$, and divide the count by the number of permutations; the result is the p-value.
\end{enumerate}

For the Steps 2 and 3, it is not necessary to actually fit the reduced model at each point in the image. The permuted dataset can equivalently be obtained as $\mathbf{Y}^{*}_{j} = \left(\mathbf{P}_{j}\mathbf{R}_{\mathbf{Z}}+\mathbf{H}_{\mathbf{Z}}\right)\mathbf{Y}$, which is particularly efficient for neuroimaging applications in the typical case of a single design matrix for all image points, as the term $\mathbf{P}_{j}\mathbf{R}_{\mathbf{Z}}+\mathbf{H}_{\mathbf{Z}}$ is then constant throughout the image and so, needs to be computed just once. Moreover, add the nuisance variables back in Step 3 is not strictly necessary, and the model can be expressed simply as $\mathbf{P}_{j}\mathbf{R}_{\mathbf{Z}}\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\mathbf{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon}$, implying that the permutations can actually be performed just by permuting the rows of the residual-forming matrix $\mathbf{R}_{\mathbf{Z}}$. The Freedman--Lane strategy is the one used in the \texttt{randomise} algorithm, discussed in \ref{sec:randomise}.

The rationale for this permutation method is that, if the null hypothesis is true, then $\boldsymbol{\beta}=\mathbf{0}$, and so the residuals from the reduced model with only nuisance variables, $\boldsymbol{\epsilon}_{\mathbf{Z}}$, should not be different than the residuals from the full model, $\boldsymbol{\epsilon}$, and can, therefore, be used to create the reference distribution from which p-values can be obtained.

The \emph{Smith procedure} consists of orthogonalising the regressors of interest with respect to the nuisance variables. This is done by pre-multiplication of $\mathbf{X}$ by the residual forming matrix due to $\mathbf{Z}$, i.e., $\mathbf{R}_{\mathbf{Z}}$, then permuting this orthogonalised version of the regressors of interest. The nuisance regressors remain in the model.\footnote{We name this method after Smith because, although orthogonalisation is a well known procedure, it does not seem to have been proposed by anyone to address the issues with permutation methods with the \textsc{glm} until Smith and others presented it in a conference poster \citep{Nichols2008}. We also use the eponym to keep it consistent with \citet{Ridgway2009}, and to keep the convention of calling the methods by the earliest author that we could identify as the proponent for each method, even though this method seems to have been proposed by an anonymous referee of \citet{OGorman2005}.}

For both the Freedman--Lane and the Smith procedures, if the errors are independent and symmetric (\textsc{ise}), the permutation matrices $\mathbf{P}_{j}$ can be replaced for sign flipping matrices $\mathbf{S}_{j}$. If both \textsc{ee} and \textsc{ise} are considered appropriate, then permutation and sign flipping can be used concomitantly.

\subsubsection{Restricted exchangeability}
\label{sec:restricted}

Some experimental designs involve multiple observations from each subject, or the subjects may come from groups that may possess characteristics that may render their distributions not perfectly comparable. Both situations violate exchangeability. However, when the dependence between observations has a block structure, this structure can be taken into account when permuting the model, restricting the set of all otherwise possible permutations to only those that respect the relationship between observations \citep{Pesarin2001}.\footnote{Observations that are exchangeable only in some subsets of all possible permutations are said \emph{weakly exchangeable} \citep{Good2002}.} The \textsc{ee} and \textsc{ise} assumptions are then asserted at the level of these exchangeability blocks, rather than for each observation individually. The experimental hypothesis and the study design determine how the \textsc{eb}s should be formed and how the permutation or sign flipping matrices should be constructed. Except Huh--Jhun, the other methods can be applied at the block level as in the unrestricted case.

\paragraph{Within-block exchangeability}

Observations that share the same dependence structure, either assumed or known in advance, can be used to define \textsc{eb}s such that \textsc{ee} are asserted with respect to these blocks only, and the empirical distribution is constructed by permuting exclusively within block, as shown in Figure~\ref{fig:within-block}. Once the blocks have been defined, the regression of nuisance variables and the construction of the reference distribution can follow strategies as Freedman--Lane or Smith, as above. The \textsc{ise}, when applicable, is transparent to this kind of block structure, so that the sign flips occur as under unrestricted exchangeability. For within-block exchangeability, in general each \textsc{eb} corresponds to a \textsc{vg} for the computation of the test statistic. See \ref{sec:examples} for examples.

\begin{figure}[!p]
\centering
\includegraphics{images/within-block.eps}
\caption[Example of permutation and sign flipping matrix for within-block exchangeability]{\emph{Left:} Example of a permutation matrix that shuffles data within block only. The blocks are not required to be of the same size. The elements outside the diagonal blocks are always equal to zero, such that data cannot be swapped across blocks. \emph{Right:} Example of a sign flipping matrix. Differently than within-block permutation matrices, here sign flipping matrices are transparent to the definitions of the blocks, such that the block definitions do not need to be taken into account, albeit their corresponding variance groups are considered when computing the statistic.}
\label{fig:within-block}
\end{figure}

\paragraph{Whole-block exchangeability}

Certain experimental hypotheses may require the comparison of sets of observations to be treated as a whole, being not exchangeable within set. Exchangeability blocks can be constructed such that each include, in a consistent order, all the observations pertaining to a given set and, differently than in within-block exchangeability, here each block is exchanged with the others on their entirety, while maintaining the order of observations within block unaltered. For \textsc{ise}, the signs are flipped for all observations within block at once. Variance groups are not constructed one per block; instead, each \textsc{vg} encompasses one or more observations per block, all in the same order, e.g., one \textsc{vg} with the first observation of each block, another with the second of each block and so on. Consequently, all blocks must be of the same size, and all with their observations ordered consistently, either for \textsc{ee} or for \textsc{ise}. Examples of permutation and sign flipping matrices for whole block permutation are shown in Figure~\ref{fig:whole-block}. See \ref{sec:examples} for examples.

\begin{figure}[!p]
\centering
\includegraphics{images/whole-block.eps}
\caption[Example of permutation and sign flipping matrix for whole-block exchangeability]{(a) Example of a permutation matrix that shuffles whole blocks of data. The blocks need to be of the same size. (b) Example of a sign flipping matrix that changes the signs of the blocks as a whole. Both matrices can be constructed by the Kronecker product (represented by the symbol $\otimes$) of a permutation or a sign flipping matrix (with size determined by the number of blocks) and an identity matrix (with size determined by the number of observations per block).}
\label{fig:whole-block}
\end{figure}

\paragraph{Variance groups mismatching exchangeability blocks} While variance groups can be defined implicity, as above, according to whether within- or whole-block permutation is to be perfomed, this is not compulsory. In some cases the \textsc{eb}s are defined based on the non-independence between observations, even if the variances across all observations can still be assumed to be identical. See \ref{sec:examples} for an example using a paired $t$-test.

\paragraph{Choice of the statistic with exchangebility blocks} The statistics $F$ and $t$, described in Section~\ref{sec:model}, are pivotal and follow known distributions when, among other assumptions, the error terms for all observations are identically distributed. Under these assumptions, all the errors terms can be pooled to compute the residual sum of squares (the term $\boldsymbol{\hat{\epsilon}}'\boldsymbol{\hat{\epsilon}}$ in the Equation~\ref{eqn:fstat}) and so, the variance of the parameter estimates. This forms the basis for parametric inference, and is also useful for non-parametric tests. However, the presence of \textsc{eb}s is incompatible with the equality of distributions across all observations, with the undesired consequence that pivotality is lost, as shown in Sections~\ref{sec:method_statistic} and \ref{sec:results_statistic}. Although these statistics can still be used with permutation methods in general, the lack of pivotality for imaging applications can cause problems for correction for multiple testing. When exchangebility blocks are present, a suitable statistic can be computed as:

\begin{equation}
G = \dfrac{\boldsymbol{\hat{\psi}}'\mathbf{C} \left(\mathbf{C}'(\mathbf{M}'\mathbf{W}\mathbf{M})^{-1}\mathbf{C} \right)^{-1} \mathbf{C}'\boldsymbol{\hat{\psi}}}{\Lambda \cdot \mathsf{rank}\left(\mathbf{C}\right)}
\label{eqn:gstat}
\end{equation}

\noindent
where $\mathbf{W}$ is a $N \times N$ diagonal weighting matrix that has elements

\begin{equation}
W_{nn} = \dfrac{\sum_{n' \in g_{n}}R_{n'n'}}{\boldsymbol{\hat{\epsilon}}_{g_{n}}'\boldsymbol{\hat{\epsilon}}_{g_{n}}}
\end{equation}

\noindent where $g_{n}$ represents the variance group to which the $n$-th observation belongs, $R_{n'n'}$ is the $n'$-th diagonal element of the residual forming matrix, and $\boldsymbol{\hat{\epsilon}}_{g_{n}}$ is the vector of residuals associated with the same \textsc{vg}.\footnote{Note that, for clarity, $G$ is defined in Equation~\ref{eqn:gstat} as a function of $\mathbf{M}$, $\boldsymbol{\psi}$ and $\mathbf{C}$ in the unpartitioned model. With the partitioning described in the \ref{sec:partitioning}, each of these variables is replaced by their equivalents in the partitioned, full model, i.e., $[\mathbf{X} \; \mathbf{Z}]$, $[\boldsymbol{\beta}' \; \boldsymbol{\gamma}']'$ and $[\mathbf{I}_{s \times s}\;\mathbf{0}_{s \times (r-s)}]'$ respectively.} In other words, each diagonal element of $\mathbf{W}$ is the reciprocal of the estimated variance for their corresponding group. This variance estimator is equivalent to the one proposed by \citet{Horn1975}. The remaining term in Equation~\ref{eqn:gstat} is given by \citep{Welch1951}:

\begin{equation}
\Lambda = 1+\frac{2(s-1)}{s(s+2)}\sum_{g} \frac{1}{\sum_{n \in g}R_{nn}} \left(1-\frac{\sum_{n \in g}W_{nn}}{\mathsf{trace}\left(\mathbf{W}\right)}\right)^2
\end{equation}

\noindent
where $s=\mathsf{rank}\left(\mathbf{C}\right)$ as before. The statistic $G$ provides a generalisation of a number of well known statistical tests, some of them summarised in Table~\ref{tab:G}. When there is only one \textsc{vg}, variance estimates can be pooled across all observations, resulting in $\Lambda=1$ and so, $G=F$. If $\mathbf{W}=\mathbf{V}^{-1}$, the inverse of the true covariance matrix, $G$ is the statistic for an $F$-test in a weighted least squares model (\textsc{wls}) \citep{Christensen2002}. If there are multiple variance groups, $G$ is equivalent to the $v^2$ statistic for the problem of testing the means for these groups under no homoscedasticity assumption, i.e., when the variances cannot be assumed to be all equal \citep{Welch1951}.\footnote{If the errors are independent and normally distributed, yet not necessarily with equal variances (i.e., $\Lambda \neq 1$), parametric p-values for $G$ can be approximated by referring to the $F$-distribution with degrees of freedom $\nu_1=s$ and $\nu_2=2(s-1)/3(\Lambda-1)$.} If, despite heteroscedasticity, $\Lambda$ is replaced by 1, $G$ is equivalent to the James' statistic for the same problem \citep{James1951}. When $\mathsf{rank}\left(\mathbf{C}\right) = 1$, and if there are more than one \textsc{vg}, $\mathsf{sign}(\boldsymbol{\hat{\beta}})\sqrt{G}$ is the well-known $v$ statistic for the Behrens--Fisher problem \citep{Fisher1935_fid, Aspin1949}; with only one \textsc{vg} present, the same expression produces the Student's $t$ statistic, as shown earlier. If the definition of the blocks is respected, all these particular cases produce pivotal statistics, and the generalisation provided by $G$ allows straightforward implementation.

\begin{table}[!t]
\caption[Some tests of which the statistic $G$ is a generalisation.]{The statistic $G$ provides a generalisation for a number of well known statistical tests.}
\begin{center}
{\small
\begin{tabular}{@{}m{70mm}<{\raggedright}@{}m{25mm}<{\centering}m{25mm}<{\centering}@{}}
\toprule
{} & $\mathsf{rank}\left(\mathbf{C}\right) = 1$ & $\mathsf{rank}\left(\mathbf{C}\right) > 1$ \\
\midrule
Homoscedastic errors, unrestricted exchangeability ($\Lambda = 1$) & Square of Student's $t$ & $F$-ratio \\
\midrule
Homoscedastic within \textsc{vg}, restricted exchangeability ($\Lambda \neq 1$) & Square of Aspin--Welch $v$ & Welch's $v^2$\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:G}
\end{table}

\subsection{Number of permutations} 

For a study with $N$ observations, the maximum number of possible permutations is $N!$, and the maximum number of possible sign flips is $2^N$. However, in the presence of $B$ exchangebility blocks that are exchangeable as a whole, the maximum number of possible permutations drops to no more than $B!$, and the maximum number of sign flips to $2^B$. For designs where data is only exchangeable within-block, the maximum number of possible permutations is $\prod_{b=1}^{B} N_{b}!$, where $N_{b}$ is the number of observations for the $b$-th block, and the maximum number of sign flips continues to be $2^N$.

However, the actual number of possible rearrangements may be smaller depending on the null hypothesis, the permutation strategy, or other aspects of the study design. If there are discrete covariates, or if there are ties among continuous regressors, many permutations may not alter the model at all. The maximum number of permutations can be calculated generically from the design matrix observing the number of repeated rows in $\mathbf{X}$ for the Freedman--Lane and most other methods, or in $\mathbf{M}$ for the ter Braak and Manly methods. The maximum number of possible permutations or sign flips, for different restrictions on exchangeability, is shown in Table~\ref{tab:numperm}.

\begin{table}[p]
\caption[Maximum number of unique permutations.]{Maximum number of unique permutations considering exchangeability blocks.}
\begin{center}
{\small
\begin{tabular}{@{}m{62mm}<{\raggedright}m{44mm}<{\centering}@{}m{15mm}<{\centering}@{}}
\toprule
Exchangeability & \textsc{ee}   & \textsc{ise}\\
\midrule
Unrestricted & \[N!\] & \[2^N\]\\
\midrule
Unrestricted, repeated rows & \[N!\prod_{m=1}^M \frac{1}{N_m!}\] & \[2^N\] \\
\midrule
Within-block & \[\prod_{b=1}^{B} N_{b}!\] & \[2^N\] \\
\midrule
Within-block, repeated rows & \[\prod_{b=1}^{B} N_{b}! \prod_{m=1}^{M|b} \frac{1}{N_{m|b}!}\] & \[2^N\] \\
\midrule
Whole-block  & \[B!\] & \[2^B\]\\
\midrule
Whole-block, repeated blocks  & \[B!\prod_{\tilde{m}=1}^{\tilde{M}} \frac{1}{N_{\tilde{m}}!}\] & \[2^B\]\\
\bottomrule
\end{tabular}}
\end{center}
{\footnotesize
\begin{enumerate}
\item [$B$] Number of exchangebility blocks (\textsc{eb}).
\item [$M$] Number of distinct rows in $\mathbf{X}$.
\item [$M|b$] Number of distinct rows in $\mathbf{X}$ within the $b$-th block.
\item [$\tilde{M}$] Number of distinct blocks of rows in $\mathbf{X}$.
\item [$N$] Number of observations.
\item [$N_b$] Number of observations in the $b$-th block.
\item [$N_m$] Number of times each of the $M$ distinct rows occurs in $\mathbf{X}$.
\item [$N_{m|b}$] Number of times each of the $m$-th unique row occurs within the $b$-th block.
\item [$N_{\tilde{m}}$] Number of times each of the $\tilde{M}$ distinct blocks occurs in $\mathbf{X}$.
\end{enumerate}}
\label{tab:numperm}
\end{table}

Even considering the restrictions dictacted by the study design, the number of possible shufflings tends to be very large, even for samples of moderate size, and grows very rapidly as observations are included. When the number of possible rearrangements is large, not all of them need to be performed for the test to be valid \citep{Dwass1957, Chung1958}, and the resulting procedure will be approximately exact \citep{Edgington1969}. The number can be chosen according to the availability of computational resources and considerations about power and precision. The smallest p-value that can be obtained is given by $1/J$, where $J$ is the number of permutations performed. The precision of permutation p-values may be determined considering the confidence interval around the significance level.

To efficiently avoid permutations that do not change the design matrix, the Algorithm ``\textsc{l}'' \citep{Knuth2005} can be used. This algorithm is simple and has the benefit of generating only permutations that are unique, i.e., in the presence of repeated elements, it correctly avoids synonymous permutations. This is appropriate when enumerating all possible permutations. However, the algorithm produces sequentially permutations that are in lexicographic order. Although this can be advantageous in other settings, here this behaviour can be problematic when running only a subset of $\mathcal{P}$, and has potential to bias the results. For imaging applications, where there are many points (voxels, vertices, faces) being analysed, it is in general computationally less expensive to shuffle many times a sequence of values and store these permuted sequences, than actually fit the permuted model for all points. As a consequence, the problem with lexicographically ordered permutations can be solved by generating all the possible permutations, and randomly drawing $J$ elements from $\mathcal{P}$ to do the actual shufflings of the model,  or generating random permutations and checking for duplicates. Alternatively, the procedure can be conducted without attention to repeated permutations using simple shuffling of the data. This strategy is known as \emph{conditional Monte Carlo} (\textsc{cmc}) \citep{Trotter1956, Pesarin2010}, as each of the random realisations is conditional on the available observed data.

Sign flipping matrices, on the other hand, can be listed using a numeral system with radix 2, and the sign flipped models can be performed without the need to enumerate all possible flips or to appeal to \textsc{cmc}. The simplest strategy is to use the digits $0$ and~$1$ of the binary numeral system, treating 0 as $-1$ when assembling the matrix. In a binary system, each sign flipping matrix is also its own numerical identifier, such that avoiding repeated sign flippings is trivial. The binary representation can be converted to and from radix 10 if needed, e.g., to allow easier human readability.

For within-block exchangeability, permutation matrices are constructed within-block, then concatenated along their diagonal to assemble $\mathbf{P}_{j}$, which also has a block structure. The elements outside the blocks are filled with zeros as needed  (Figure~\ref{fig:within-block}). The block definitions can be ignored for sign flipping matrices for designs where \textsc{ise} is asserted within-block. For whole-block exchangeability, permutation and sign flipping matrices are generated by treating each block as an element, and the final $\mathbf{P}_{j}$ or $\mathbf{S}_{j}$ are assembled via Kronecker multiplication by an identity matrix of the same size as the blocks (Figure~\ref{fig:whole-block}).

\subsection{Multiple testing}

Differently than with parametric methods, correction for multiple testing using permutation does not require the introduction of more assumptions. For familywise error rate correction (\textsc{fwer}), the method was described by \citet{Holmes1996}. As the statistics $T_{j}^{*}$ are calculated for each shuffling to build the reference distribution at each point, the maximum value of $T_{j}^{*}$ across the image, $T_{j}^{\text{max}}$, is also recorded for each rearrangement, and its empirical distribution is obtained. For each test in the image, an \textsc{fwer}-corrected p-value can then be obtained by computing the proportion of $T_{j}^{\text{max}}$ that is above $T_{0}$ for each test. A single \textsc{fwer} threshold can also be applied to the statistical map of $T_{0}$ values using the distribution of $T_{j}^{\text{max}}$. The same strategy can be used for statistics that combine spatial extent of signals, such as cluster extent or mass \citep{Bullmore1999}, threshold-free cluster enhancement (\textsc{tfce}) \citep{Smith2009} and others \citep{Marroquin2011}. For these spatial statistics, the effect of lack of pivotality can be mitigated by non-stationarity correction \citep{Hayasaka2004, Salimi-Khorshidi2011}.

The p-values under the null hypothesis are uniformly distributed in the interval $[0,1]$. As a consequence, the p-values themselves are
pivotal quantities and, in principle, could be used for multiple testing correction as above. The distribution of minimum p-value, $p_{j}^{\text{min}}$, instead of $T_{j}^{\text{max}}$, can be used. Due to the discreteness of the p-values, this approach, however, entails some computational difficulties that may cause considerable loss of power \citep{Pantazis2005}. Correction based on false-discovery rate (\textsc{fdr}) can be used once the uncorrected p-values have been obtained for each point in the image. Either a single \textsc{fdr} threshold can be applied to the map of uncorrected p-values \citep{Benjamini1995, Genovese2002} or an \textsc{fdr}-adjusted p-value can be calculated at each point \citep{Yekutieli1999}.

\subsection{The \texttt{randomise} algorithm}
\label{sec:randomise}

Algorithm~1 describes a procedure for permutation inference on contrasts of the \textsc{glm} parameter estimates using the Freedman--Lane method. Modifications for other methods are trivial. For this algorithm, consider $\mathbf{Y}$ as a four-dimensional array, being the first three dimensions for space and the last for an observation index. A variable $\mathbf{v}=[x, y, z]$ is used to specify the point position in space, so that the vector of $n$ different observations per point is represented as $\mathbf{Y}[\mathbf{v}]$. A set $\mathcal{C}$ of contrasts is specified, as well as the unpartitioned design matrix $\mathbf{M}$. Indicator variables are used to specify whether the errors should be treated as exchangeable ($\textsc{ee}=\textsc{true}$), independent and symmetric ($\textsc{ise}=\textsc{true}$), or both, which allows for permutations to happen together with sign flipping. A positive integer $J$ is specified as the number permutations to be performed. Optionally, a $n \times 1$ vector $\mathbf{b}$ is provided to indicate the $B$ exchangebility blocks that group the observations, along with an indicator variable $\textsc{pb}$ that informs whether blocks should be permuted as a whole ($\textsc{pb} = \textsc{true}$), or if permutations should happen within block only ($\textsc{pb} = \textsc{false}$). The specification of $\mathbf{b}$ and \textsc{pb} obviate the need to specify the variance groups, as these can be defined implicitly for within or whole-block permutation when the pivotal statistic is computed.

\vspace{4mm}
\singlespacing
\noindent \textsf{Algorithm 1: The \texttt{randomise} algorithm.}\\
\HRule
\vspace{1mm}
{\small
\begin{algorithmic}[1]
\Require $\mathbf{Y}, \mathbf{M}, \mathcal{C}, \textsc{ee}, \textsc{ise}, J$. \textbf{Optional:} $\mathbf{b}, \textsc{pb}$.
\Comment{Input variables.}
\If{\textlnot\ \textsf{exist}(\textsc{pb})}
\Comment{If \textsc{pb} was not provided.}
\State $\textsc{pb} \leftarrow \textsc{false}$
\Comment{Permutations happen within block.}
\EndIf
\If{\textlnot\ \textsf{exist}($\mathbf{b}$)}
\Comment{If $\mathbf{b}$ was not provided.}
\State $\mathbf{b} \leftarrow \mathbf{1}_{n \times 1}$
\Comment{A vector of ones is used for $\mathbf{b}$.}
\State $\textsc{pb} \leftarrow \textsc{false}$
\Comment{Permutations happen within the single block.}
\EndIf
\ForAll{$\mathbf{C} \in \mathcal{C}$}
\Comment{For each contrast.}
\State $\mathbf{X}, \mathbf{Z} \leftarrow \text{\textsf{partition}}(\mathbf{M}, \mathbf{C})$
\Comment{Partition the model.}
\State $\mathbf{M} \leftarrow [\mathbf{X}\;\mathbf{Z}]$
\Comment{For simplicity, replace $\mathbf{M}$.}
\State $J^{\text{max}} \leftarrow \text{\textsf{calc\_maxshuf}}(\mathbf{X}, \mathbf{b}, \textsc{pb}, \textsc{ee}, \textsc{ise})$
\Comment{Maximum possible shufflings.}
\If {$\textsc{ee}$}
\Comment{If errors are exchangeable.}
\If {$J \geqslant J^{\text{max}}$}
\Comment{Exhaustive or too many permutations requested.}
\State $\mathcal{P} \leftarrow \text{\textsf{algorithm\_L}}(\mathbf{X}, \mathbf{b}, \textsc{pb})$
\Comment{List all possible permutations.}
\Else
\State $\mathcal{P} \leftarrow \text{\textsf{permute\_randomly}}(\mathbf{X}, \mathbf{b}, \textsc{pb}, J-1)$
\Comment{Ignore repeated $\mathbf{P}_{j}$.}
\State $\mathcal{P} \leftarrow \{\mathcal{P},\mathbf{I}\}$
\Comment{Ensure inclusion of the unpermuted model.}
\EndIf
\EndIf
\If {$\textsc{ise}$}
\Comment{If errors are independent and symmetric.}
\If {$J \geqslant J^{\text{max}}$}
\Comment{Exhaustive or too many sign flips requested.}
\State $\mathcal{S} \leftarrow \text{\textsf{list\_signflips}}(\mathbf{b}, \textsc{pb})$
\Comment{List all possible sign flippings.}
\Else
\State $\mathcal{S} \leftarrow \text{\textsf{signflip\_randomly}}(n, \mathbf{b}, \textsc{pb}, J-1)$
\Comment{Ignore repeated $\mathbf{S}_{j}$.}
\State $\mathcal{S} \leftarrow \{\mathcal{S},\mathbf{I}\}$
\Comment{Ensure inclusion of the non-sign flipped model.}
\EndIf
\EndIf
\If {$\textsc{ee}$ $\wedge$ $\textsc{ise}$}
\Comment{Errors independent, symmetric and exchangeable.}
\State $\mathcal{B} \leftarrow \text{\textsf{draw\_products}}(\mathcal{P}, \mathcal{S}, J)$
\Comment{Draw $J$ random products $\mathbf{P}_{j'}\mathbf{S}_{j''}$.}
\If {$\mathbf{I} \notin \mathcal{B}$}
\Comment{If non-shuffled model is absent from $\mathcal{B}$.}
\State $\mathcal{B} \leftarrow \{\mathbf{B}_{1},\ldots,\mathbf{B}_{J-1},\mathbf{I}\}$
\Comment{Ensure non-shuffled model is included.}
\EndIf
\State $\mathcal{B} \leftarrow \mathcal{P}$
\Comment{Treat $\mathcal{B}$ as $\mathcal{P}$ for simplicity.}
\ElsIf {$\textsc{ise}$ $\wedge$ \textlnot\ $\textsc{ee}$}
\Comment{If errors are only independent and symmetric.}
\State $\mathcal{P} \leftarrow \mathcal{S}$
\Comment{Treat $\mathcal{S}$ as $\mathcal{P}$ for simplicity.}
\EndIf
\ForAll{$\mathbf{v}$}
\Comment{For each image point.}
\State $\mathbf{U}[\mathbf{v}] \leftarrow 0$
\Comment{Initialise counter for uncorrected p-value.}
\State $\mathbf{F}[\mathbf{v}] \leftarrow 0$
\Comment{Initialise counter for \textsc{fwer}-corrected p-value.}
\State $\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}[\mathbf{v}] \leftarrow (\mathbf{I}-\mathbf{Z}\mathbf{Z}^{+})\mathbf{Y}[\mathbf{v}]$
\Comment{Remove the nuisance effects.}
\State $\boldsymbol{\hat{\psi}}[\mathbf{v}] \leftarrow \mathbf{M}^{+}\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}[\mathbf{v}]$
\Comment{Estimate regression coefficients.}
\State $\boldsymbol{\hat{\epsilon}}[\mathbf{v}] \leftarrow (\mathbf{I}-\mathbf{M}\mathbf{M}^{+})\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}[\mathbf{v}]$
\Comment{Estimate the residuals.}
\State $\mathbf{T}_{0}[\mathbf{v}] \leftarrow \text{\textsf{pivotal}}(\boldsymbol{\hat{\psi}}[\mathbf{v}],\boldsymbol{\hat{\epsilon}}[\mathbf{v}],\mathbf{M},\mathbf{b},\text{\textsc{pb}})$
\Comment{Compute a pivotal statistic.}
\EndFor
\For{$\mathbf{P}_{j} \in \mathcal{P}$}
\Comment{For each shuffling (permutation and/or sign flipping).}
\State $\mathbf{M}_{j}^{*} \leftarrow \mathbf{P}_{j}\mathbf{M}$
\Comment{Shuffle the model.}
\ForAll{$\mathbf{v}$}
\Comment{For each image point.}
\State $\boldsymbol{\hat{\psi}}_{j}^{*}[\mathbf{v}] \leftarrow (\mathbf{M}_{j}^{*})^{+}\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}[\mathbf{v}]$
\Comment{Fit permuted model.}
\State $\boldsymbol{\hat{\epsilon}}_{j}^{*}[\mathbf{v}] \leftarrow (\mathbf{I}-\mathbf{M}_{j}^{*}(\mathbf{M}_{j}^{*})^{+})\boldsymbol{\hat{\epsilon}}_{\mathbf{Z}}[\mathbf{v}]$
\Comment{Residuals.}
\State $\mathbf{T}_{j}^{*}[\mathbf{v}] \leftarrow \text{\textsf{pivotal}}(\boldsymbol{\hat{\psi}}_{j}^{*}[\mathbf{v}],\boldsymbol{\hat{\epsilon}}_{j}^{*}[\mathbf{v}],\mathbf{M}_{j}^{*},\mathbf{b},\text{\textsc{pb}})$
\Comment{Shuffled statistic.}
\If{$\mathbf{T}_{j}^{*}[\mathbf{v}] \geqslant \mathbf{T}_{0}[\mathbf{v}]$}
\Comment{If shuffled statistic is larger.}
\State $\mathbf{U}[\mathbf{v}] \leftarrow \mathbf{U}[\mathbf{v}]+1$
\Comment{Increment counter for uncorrected.}
\EndIf
\EndFor
\State $T^{\text{max}}_{j} \leftarrow \text{\textsf{max}}(\mathbf{T}_{j}^{*})$
\Comment{Find the largest $T_{j}^{*}$ across space.}
\ForAll{$\mathbf{v}$}
\Comment{For each image point.}
\If{$T^{\text{max}}_{j} \geqslant \mathbf{T}_{0}[\mathbf{v}]$}
\Comment{If $T_{j}^{\text{max}}$ is larger.}
\State $\mathbf{F}[\mathbf{v}] \leftarrow \mathbf{F}[\mathbf{v}]+1$
\Comment{Increment counter for \textsc{fwer}-corrected.}
\EndIf
\EndFor
\EndFor
\State p-value $\leftarrow \mathbf{U} / J$
\Comment{Significance map for this $\mathbf{C}$, uncorrected.}
\State p$_{\text{\textsc{fwer}}}$-value $\leftarrow \mathbf{F} / J$
\Comment{Significance map for this $\mathbf{C}$, \textsc{fwer}-corrected.}
\State \Return p-value, p$_{\text{\textsc{fwer}}}$-value.
\Comment{Save significance images to disk.}
\EndFor
\end{algorithmic}}
\noindent
\HRule\\
\setstretch{\lspac}
\vspace{0mm}

In the algorithm, the statistics $T$ for each point (voxel, vertex, face) are stored in the array $\mathbf{T}$, whereas the counters are stored in the arrays $\mathbf{U}$ and $\mathbf{F}$. The design matrix as well as the contrasts can be specific for each image point (voxelwise, vertexwise, facewise), and there is no challenge other than implementation. It is possible to omit the for-loop between lines 56 and 60, and instead store the distribution of the largest statistic as a vector of size $J$, which is then used to assess significance. The code runs faster, but it would be slightly less clear to present. In programming languages that offer good matrix manipulation capabilities, e.g.\ Octave, \textsc{matlab} or \textsc{r}, the for-loops that iterate for each point $\mathbf{v}$ can be replaced by matrix operations that are executed all in a single step. In the \textsc{Fmrib} Software Library (\textsc{fsl})\footnote{Available for download at \href{http://www.fmrib.ox.ac.uk/fsl}{\texttt{http://www.fmrib.ox.ac.uk/fsl}}.}, a fast implementation, in \textsc{c}++, of the \texttt{randomise} algorithm is available.


\section{Evaluation methods}

\subsection{Choice of the statistic}
\label{sec:method_statistic}

We conducted extensive simulations to study the behaviour of the common $F$ statistic (Equation~\ref{eqn:fstat}) as well as of the generalised $G$ statistic (Equation~\ref{eqn:gstat}), proposed here for use in neuroimaging, in various scenarios of balanced and unbalanced designs and variances for the variance groups. Some of the most representative of these scenarios are shown in Table~\ref{tab:kolmo}. The main objective of the simulations was to assess whether these statistics would retain their distributions when the variances are not equal for each sample. Within each scenario, 3 or 5 different configurations of simulated variances were tested, pairwise, for the equality of distributions using the two-sample Kolmogorov--Smirnov test (\textsc{ks}) \citep{Press1992}, with a signficance level $\alpha = 0.05$, corrected for multiple testing within each scenario using the Bonferroni correction, as these tests are independent.

\begin{table}[!p]
\caption[Summary of simulation scenarios.]{The eight different simulation scenarios, each with its own same sample sizes and different variances. The distributions of the statistic ($F$ or $G$) for each pair of variance configuration within scenario were compared using the \textsc{ks} test. The letters in the last column (marked with a star, $\star$) indicate the variance configurations represented in the pairwise comparisons shown in Figure~\ref{fig:kolmo} and results shown in Table~\ref{tab:comparefg}.}
\begin{center}
{\small
\begin{tabular}{@{}m{20mm}<{\raggedright}m{28mm}<{\raggedright}m{23mm}<{\raggedright}m{10mm}<{\centering}@{}}
\toprule
Simulation scenario & Sample sizes for each \textsc{vg} & Variances for each \textsc{vg} & $\star$\\
\midrule
\multirow{5}{*}{1} & \multirow{5}{*}{8, 4}   & 5, 1   & (\emph{a}) \\
{}                 & {}                      & 1.2, 1 & (\emph{b}) \\
{}                 & {}                      & 1, 1   & (\emph{c}) \\
{}                 & {}                      & 1, 1.2 & (\emph{d}) \\
{}                 & {}                      & 1, 5   & (\emph{e}) \\
\midrule
\multirow{5}{*}{2} & \multirow{5}{*}{20, 5}  & 5, 1   & (\emph{a}) \\
{}                 & {}                      & 1.2, 1 & (\emph{b}) \\
{}                 & {}                      & 1, 1   & (\emph{c}) \\
{}                 & {}                      & 1, 1.2 & (\emph{d}) \\
{}                 & {}                      & 1, 5   & (\emph{e}) \\
\midrule
\multirow{5}{*}{3} & \multirow{5}{*}{80, 30} & 5, 1   & (\emph{a}) \\
{}                 & {}                      & 1.2, 1 & (\emph{b}) \\
{}                 & {}                      & 1, 1   & (\emph{c}) \\
{}                 & {}                      & 1, 1.2 & (\emph{d}) \\
{}                 & {}                      & 1, 5   & (\emph{e}) \\
\midrule
\multirow{5}{*}{4} & \multirow{5}{*}{40, 30, 20, 10} & 15, 10, 5, 1     & (\emph{a}) \\
{}                 & {}                              & 3.6, 2.4, 1.2, 1 & (\emph{b}) \\
{}                 & {}                              & 1, 1, 1, 1       & (\emph{c}) \\
{}                 & {}                              & 1, 1.2, 2.4, 3.6 & (\emph{d}) \\
{}                 & {}                              & 1, 5, 10, 15     & (\emph{e}) \\
\midrule
\multirow{3}{*}{5} & \multirow{3}{*}{4, 4}   & 1, 1   & (\emph{a}) \\
{}                 & {}                      & 1, 1.2 & (\emph{b}) \\
{}                 & {}                      & 1, 5   & (\emph{c}) \\
\midrule
\multirow{3}{*}{6} & \multirow{3}{*}{20, 20} & 1, 1   & (\emph{a}) \\
{}                 & {}                      & 1, 1.2 & (\emph{b}) \\
{}                 & {}                      & 1, 5   & (\emph{c}) \\
\midrule
\multirow{3}{*}{7} & \multirow{3}{*}{4, 4, 4, 4} & 1, 1, 1, 1       & (\emph{a}) \\
{}                 & {}                          & 1, 1.2, 2.4, 3.6 & (\emph{b}) \\
{}                 & {}                          & 1, 5, 10, 15     & (\emph{c}) \\
\midrule
\multirow{3}{*}{8} & \multirow{3}{*}{20, 20, 20, 20} & 1, 1, 1, 1       & (\emph{a}) \\
{}                 & {}                              & 1, 1.2, 2.4, 3.6 & (\emph{b}) \\
{}                 & {}                              & 1, 5, 10, 15     & (\emph{c}) \\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:kolmo}
\end{table}

For each variance configuration, 1000 voxels containing normally distributed random noise, with zero expected mean, were simulated and tested for the null hypothesis of no difference between the means of the groups. The empirical distribution of the statistic for each configuration was obtained by pooling the results for the simulated voxels, then compared with the \textsc{ks} test. The process was repeated 1000 times, and the number of times in which the distributions were found to be significantly different from the others in the scame scenario was recorded. Confidence intervals (95\%) were computed using the Wilson method \citep{Wilson1927}.

By comparing the distributions of the same statistic obtained in different variance settings, this evaluation strategy mimics what is observed when the variances for each voxel varies across space in the same imaging experiment. The statistic must be robust to these differences and retain its distributional properties, even if assessed non-parametrically, otherwise \textsc{fwer} using the distribution of the maximum statistic is compromised. The same applies for multiple testing that combines more than one imaging modality.

In addition, the same scenarios and variance configurations were used to assess the proportion of error type \textsc{i} and the power of the $F$ and $G$ statistics. To assess power, a simulated signal was added to each of the groups; for the scenarios with two groups, the true $\boldsymbol{\psi}$ was defined as $[0$ $-1]'$, whereas for the scenarios with four groups, it was defined as $[0$  $-0.33$ $-0.67$ $-1]'$. In either case, the null hypothesis was that the group means were all equal. Significance values were computed using 1000 permutations, with $\alpha=0.05$, and 95\% confidence intervals were calculated using the Wilson method.

\subsection{Permutation strategies}
\label{sec:method_perm}

We compared the 10 methods described in Table~\ref{tab:methods} simulating different regression scenarios. The design considered one regressor of interest, $\mathbf{x}_{1}$, and two regressors of no interest, $\mathbf{z}_{1}$ and $\mathbf{z}_{2}$, $\mathbf{z}_{2}$ being a column-vector of just ones (intercept). The simulation scenarios considered different sample sizes, $n=\{12,$ $24,$ $48,$ $96\}$; different combinations for continuous and categorical $\mathbf{x}_{1}$ and $\mathbf{z}_{1}$; different degrees of correlation between $\mathbf{x}_{1}$ and $\mathbf{z}_{1}$, $\rho = \{0,$ $0.8\}$; different sizes for the regressor of interest, $\beta_{1}=\{0,$ $0.5\}$; and different distributions for the error terms, $\boldsymbol{\epsilon}$, as normal ($\mu=0$, $\sigma^2=1$), uniform ($\left[-\sqrt{3},\; +\sqrt{3}\;\right]$), exponential ($\lambda=1$) and Weibull ($\lambda=1$, $k=1/3$). The coefficients for the first regressor of no interest and for the intercept were kept constant as $\gamma_{1}=0.5$ and $\gamma_{2}=1$ respectively, and the distributions of the errors were shifted or scaled as needed to have expected zero mean and expected unit variance.

The continuous regressors were constructed as a linear trend ranging from $-1$ to $+1$ for $\mathbf{x}_1$, and the square of this trend, mean-centered, for $\mathbf{z}_1$. For this symmetric range around zero for $\mathbf{x}_1$, this procedure causes $\mathbf{x}_1$ and $\mathbf{z}_1$ to be orthogonal and uncorrelated. For the discrete regressors, a vector of $n/2$ ones and $n/2$ negative ones was used, the first $n/2$ values being only $+1$ and the remaining $-1$ for $\mathbf{x}_1$, whereas for $\mathbf{z}_1$, the first and last $n/4$ were $-1$ and the $n/2$ middle values were $+1$. This procedure also causes $\mathbf{x}_1$ and $\mathbf{z}_1$ to be orthogonal and uncorrelated. For each different configuration, 1000 simulated vectors $\mathbf{Y}$ were constructed as $\mathbf{Y}=[\mathbf{x}_{1} \; \mathbf{z}_{1} \; \mathbf{z}_{2}][\beta_1 \; \gamma_1 \; \gamma_2]'+\boldsymbol{\epsilon}$.

Correlation was introduced in the regression models through Cholesky decomposition of the desired correlation matrix $\mathbf{K}$, such that $\mathbf{K}=\mathbf{L}'\mathbf{L}$, then defining the regressors by multiplication by $\mathbf{L}$, i.e., $[\mathbf{x}_{1}^{\rho} \; \mathbf{z}_{1}^{\rho}] = [\mathbf{x}_{1} \; \mathbf{z}_{1}]\mathbf{L}$. The unpartitioned design matrix was constructed as $\mathbf{M}=[\mathbf{x}_{1}^{\rho}$  $\mathbf{z}_{1}^{\rho}$ $\mathbf{z}_{2}]$. A contrast $\mathbf{C}=[1 \; 0 \; 0]'$ was defined to test the null hypothesis $\mathcal{H}_{0} : \mathbf{C}'\boldsymbol{\psi} = \beta_{1} = 0$. This contrast tests only the first column of the design matrix, so partitioning $\mathbf{M}=[\mathbf{X} \; \mathbf{Z}]$ using the scheme shown in \ref{sec:partitioning} might seem unnecessary. However, we wanted to test also the effect of non-orthogonality between columns of the design matrix for the different permutation methods, with and without the more involved partitioning scheme shown in the Appendix. Permutation, sign flipping, and permutation with sign flipping were tested. Up to 1000 permutations and/or sign flippings were performed using \textsc{cmc}, being less when the maximum possible number of shufflings was not large enough. In these cases, all the permutations and/or sign flippings were performed exhaustively.

Error type~\textsc{i} was computed using $\alpha=0.05$ for configurations that used $\beta_{1}=0$. The other configurations were used to examine power. As previously, confidence intervals (95\%) were estimated using the Wilson method.

\section{Results}

\subsection{Choice of the statistic}
\label{sec:results_statistic}

Figure~\ref{fig:kolmo} shows heatmaps with the results of the pairwise comparisons between variance configurations, within each of the simulation scenarios presented in Table~\ref{tab:kolmo}, using either $F$ or $G$ statistic. For unbalanced scenarios with only two samples (simulation scenarios 1 to 3), and with modest variance differences between groups (configurations $b$ to $d$), the $F$ statistic often retained its distributional properties, albeit less often than the $G$ statistic. For large variance differences, however, this relative stability was lost for $F$, but not for $G$ ($a$ and $e$). Moreover, the inclusion of more groups (scenario 4), with unequal sample sizes, caused the distribution of the $F$ statistic to be much more sensitive to heteroscedasticity, such that almost always the \textsc{ks} test identified different distributions across different variance configurations. The $G$ statistic, on the other hand, remained robust to heteroscedasticity even in these cases.

\begin{figure}[!p]
\centering
\includegraphics[width=12.6cm]{images/kolmo.eps}
\caption[Heatmaps for evaluation of pivotality for $F$ and $G$ statistics.]{Heatmaps for the comparison of the distributions obtained under different variance settings for identical sample sizes. In each map, the cells below the main diagonal contain the results for the pairwise $F$ statistic, and above, for the $G$ statistic. The percentages refer to the fraction of the 1000 tests in which the distribution of the statistic for one variance setting was found different than for another in the same simulation scenario. Each variance setting is indicated by letters (\emph{a}--\emph{e}), corresponding to the same letters in Table~\ref{tab:kolmo}. Smaller percentages indicate robustness of the statistic to heteroscedasticity. Confidence intervals (95\%) are shown in parenthesis.}
\label{fig:kolmo}
\end{figure}

In balanced designs, either with two (simulation scenarios 5 and 6) or more (scenarios 7 and 8) groups, the $F$ statistic had a better behaviour than in unbalanced cases. For two samples of the same size, there is no difference between $F$ and $G$. For more than two groups, the $G$ statistic behaved consistently better than $F$, particularly for large variance differences.

These results suggest that the $G$ statistic is more appropriate under heteroscedasticity, with balanced or unbalanced designs, as it preserves its distributional properties, indicating more adequacy for use with neuroimaging. The $F$ statistic, on the other hand, does not preserve pivotality and can, nonetheless, be used under heteroscedasticity when the groups have the same size.

With respect to error type \textsc{i}, both $F$ and $G$ resulted in similar amount of false positives when assessed non-parametrically. The $G$ yielded generally higher power than $F$, particularly in the presence of heteroscedasticity and with unequal sample sizes. These results are presented in Table~\ref{tab:comparefg}.

\begin{table}[!p]
\caption[False positive rate and power for the statistics $F$ and $G$.]{Proportion of error type I and power (\%) for the statistics $F$ and $G$ in the various simulation scenarios and variance configurations shown in Table~\ref{tab:kolmo}. Confidence intervals (95\%) are shown in parenthesis.}
\begin{center}
{\small
\begin{tabular}{@{}m{18mm}<{\raggedright}m{8mm}<{\centering}m{20mm}<{\centering}m{20mm}<{\centering}m{20mm}<{\centering}m{20mm}<{\centering}@{}}
\toprule
\multirow{2}{*}{\parbox{18mm}{\vspace{4pt}Simulation scenario}} &\multirow{2}{*}{\raisebox{-2pt}{$\star$}} & \multicolumn{2}{c}{Proportion of error type \textsc{i}} & \multicolumn{2}{c}{Power}\\
\cmidrule(lr){3-4} \cmidrule(l){5-6}
{} & {} & $F$ & $G$ & $F$ & $G$\\
\midrule
\multirow{5}{*}{1} & (\emph{a}) & 5.9 \scalebox{.7}[1.0]{(4.6--7.5)} & 6.1 \scalebox{.7}[1.0]{(4.8--7.8)} & 20.1 \scalebox{.7}[1.0]{(17.7--22.7)} & 23.8 \scalebox{.7}[1.0]{(21.3--26.5)}\\
{}                 & (\emph{b}) & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 28.3 \scalebox{.7}[1.0]{(25.6--31.2)} & 31.9 \scalebox{.7}[1.0]{(29.1--34.9)}\\
{}                 & (\emph{c}) & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)} & 29.3 \scalebox{.7}[1.0]{(26.6--32.2)} & 32.6 \scalebox{.7}[1.0]{(29.8--35.6)}\\
{}                 & (\emph{d}) & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 29.9 \scalebox{.7}[1.0]{(27.1--32.8)} & 32.0 \scalebox{.7}[1.0]{(29.2--35.0)}\\
{}                 & (\emph{e}) & 3.9 \scalebox{.7}[1.0]{(2.9--5.3)} & 4.1 \scalebox{.7}[1.0]{(3.0--5.5)} & 14.0 \scalebox{.7}[1.0]{(12.0--16.3)} & 14.1 \scalebox{.7}[1.0]{(12.1--16.4)}\\
\midrule
\multirow{5}{*}{2} & (\emph{a}) & 6.7 \scalebox{.7}[1.0]{(5.3--8.4)} & 6.6 \scalebox{.7}[1.0]{(5.2--8.3)} & 29.1 \scalebox{.7}[1.0]{(26.4--32.0)} & 38.3 \scalebox{.7}[1.0]{(35.3--41.4)}\\
{}                 & (\emph{b}) & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 42.4 \scalebox{.7}[1.0]{(39.4--45.5)} & 48.8 \scalebox{.7}[1.0]{(45.7--51.9)}\\
{}                 & (\emph{c}) & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)} & 44.6 \scalebox{.7}[1.0]{(41.6--47.7)} & 48.9 \scalebox{.7}[1.0]{(45.8--52.0)}\\
{}                 & (\emph{d}) & 6.1 \scalebox{.7}[1.0]{(4.8--7.8)} & 6.2 \scalebox{.7}[1.0]{(4.9--7.9)} & 42.3 \scalebox{.7}[1.0]{(39.3--45.4)} & 46.7 \scalebox{.7}[1.0]{(43.6--49.8)}\\
{}                 & (\emph{e}) & 5.9 \scalebox{.7}[1.0]{(4.6--7.5)} & 6.2 \scalebox{.7}[1.0]{(4.9--7.9)} & 19.5 \scalebox{.7}[1.0]{(17.2--22.1)} & 19.0 \scalebox{.7}[1.0]{(16.7--21.6)}\\
\midrule
\multirow{5}{*}{3} & (\emph{a}) & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 90.4 \scalebox{.7}[1.0]{(88.4--92.1)} & 92.3 \scalebox{.7}[1.0]{(90.5--93.8)}\\
{}                 & (\emph{b}) & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 99.7 \scalebox{.7}[1.0]{(99.1--99.9)} & 99.8 \scalebox{.7}[1.0]{(99.3--100)}\\
{}                 & (\emph{c}) & 6.3 \scalebox{.7}[1.0]{(5.0--8.0)} & 6.2 \scalebox{.7}[1.0]{(4.9--7.9)} & 99.8 \scalebox{.7}[1.0]{(99.3--100)}  & 99.8 \scalebox{.7}[1.0]{(99.3--100)}\\
{}                 & (\emph{d}) & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 99.6 \scalebox{.7}[1.0]{(99.0--99.8)} & 99.6 \scalebox{.7}[1.0]{(99.0--99.8)}\\
{}                 & (\emph{e}) & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 72.9 \scalebox{.7}[1.0]{(70.1--75.6)} & 72.9 \scalebox{.7}[1.0]{(70.1--75.6)}\\
\midrule
\multirow{5}{*}{4} & (\emph{a}) & 6.4 \scalebox{.7}[1.0]{(5.0--8.1)} & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 10.2 \scalebox{.7}[1.0]{(8.5--12.2)}  & 19.4 \scalebox{.7}[1.0]{(17.1--22.0)}\\
{}                 & (\emph{b}) & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 37.8 \scalebox{.7}[1.0]{(34.9--40.9)} & 45.6 \scalebox{.7}[1.0]{(42.5--48.7)}\\
{}                 & (\emph{c}) & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 72.2 \scalebox{.7}[1.0]{(69.3--74.9)} & 74.9 \scalebox{.7}[1.0]{(72.1--77.5)}\\
{}                 & (\emph{d}) & 3.1 \scalebox{.7}[1.0]{(2.2--4.4)} & 3.7 \scalebox{.7}[1.0]{(2.7--5.1)} & 34.6 \scalebox{.7}[1.0]{(31.7--37.6)} & 44.6 \scalebox{.7}[1.0]{(41.6--47.7)}\\
{}                 & (\emph{e}) & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)} & 4.2 \scalebox{.7}[1.0]{(3.1--5.6)} &  9.7 \scalebox{.7}[1.0]{(8.0--11.7)}  & 15.7 \scalebox{.7}[1.0]{(13.6--18.1)}\\
\midrule
\multirow{3}{*}{5} & (\emph{a}) & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 29.9 \scalebox{.7}[1.0]{(27.1--32.8)} & 29.9 \scalebox{.7}[1.0]{(27.1--32.8)}\\
{}                 & (\emph{b}) & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 30.6 \scalebox{.7}[1.0]{(27.8--33.5)} & 30.6 \scalebox{.7}[1.0]{(27.8--33.5)}\\
{}                 & (\emph{c}) & 6.9 \scalebox{.7}[1.0]{(5.5--8.6)} & 6.9 \scalebox{.7}[1.0]{(5.5--8.6)} & 14.5 \scalebox{.7}[1.0]{(12.5--16.8)} & 14.5 \scalebox{.7}[1.0]{(12.5--16.8)}\\
\midrule
\multirow{3}{*}{6} & (\emph{a}) & 3.3 \scalebox{.7}[1.0]{(2.4--4.6)} & 3.3 \scalebox{.7}[1.0]{(2.4--4.6)} & 92.6 \scalebox{.7}[1.0]{(90.8--94.1)} & 92.6 \scalebox{.7}[1.0]{(90.8--94.1)}\\
{}                 & (\emph{b}) & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 90.5 \scalebox{.7}[1.0]{(88.5--92.2)} & 90.5 \scalebox{.7}[1.0]{(88.5--92.2)}\\
{}                 & (\emph{c}) & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 53.7 \scalebox{.7}[1.0]{(50.6--56.8)} & 53.7 \scalebox{.7}[1.0]{(50.6--56.8)}\\
\midrule
\multirow{3}{*}{7} & (\emph{a}) & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 5.5 \scalebox{.7}[1.0]{(4.3--7.1)} & 11.0 \scalebox{.7}[1.0]{(9.2--13.1)}  &  8.8 \scalebox{.7}[1.0]{(7.2--10.7)}\\
{}                 & (\emph{b}) & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} &  6.5 \scalebox{.7}[1.0]{(5.1--8.2)}   &  7.8 \scalebox{.7}[1.0]{(6.3--9.6)}\\
{}                 & (\emph{c}) & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} &  5.8 \scalebox{.7}[1.0]{(4.5--7.4)}   &  6.9 \scalebox{.7}[1.0]{(5.5--8.6)}\\
\midrule
\multirow{3}{*}{8} & (\emph{a}) & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)} & 78.7 \scalebox{.7}[1.0]{(76.1--81.1)} & 78.1 \scalebox{.7}[1.0]{(75.4--80.6)}\\
{}                 & (\emph{b}) & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 40.7 \scalebox{.7}[1.0]{(37.7--43.8)} & 45.5 \scalebox{.7}[1.0]{(42.4--48.6)}\\
{}                 & (\emph{c}) & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 11.6 \scalebox{.7}[1.0]{(9.8--13.7)}  & 19.3 \scalebox{.7}[1.0]{(17.0--21.9)}\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:comparefg}
\end{table}


\subsection{Permutation strategies}
\label{sec:results_perm}

The different simulation parameters allowed 1536 different regression scenarios, being 768 without signal and 768 with signal; a summary is shown in Table~\ref{tab:summary}, and some of the most representative in Table~\ref{tab:methods_resultsC}. In ``well behaved'' scenarios, i.e., large number of observations, orthogonal regressors and normally distributed errors, all methods tended to behave generally well, with adequate control over type~\textsc{i} error and fairly similar power. However, performance differences between the permutation strategies shown in Table~\ref{tab:methods} became more noticeable as the sample sizes were decreased and skewed errors were introduced.

\begin{table}[!tp]
\caption[Summary of the amount of error type \textsc{i} and power for the different permutation strategies.]{A summary of the results for the 1536 simulations with different parameters. The amount of error type \textsc{i} is calculated for the 768 simulations without signal ($\beta_1$ = 0), whereas the power was calculated for the remaining 768 simulations with signal ($\beta_1$ = 0.5). Confidence intervals (\textsc{ci}) are at 95\%.}
\begin{center}
{\small
\begin{tabular}{@{}m{37mm}<{\raggedright}m{18mm}<{\centering}m{18mm}<{\centering}m{18mm}<{\centering}m{18mm}<{\centering}@{}}
\toprule
\multirow{2}{*}{\raisebox{-3pt}{Method}} &   \multicolumn{3}{c}{Proportion of error type \textsc{i}} & \multirow{2}{19mm}{\raisebox{-3pt}{Average} power}\\
\cmidrule(lr){2-4}
                        & Within \textsc{ci} & Below \textsc{ci} & Above \textsc{ci}\\
\midrule
Draper--Stoneman & 86.33\% &  8.20\% &  5.47\% & 72.96\%\\
Still--White     & 67.84\% & 14.58\% & 17.58\% & 71.82\%\\
Freedman--Lane   & 88.67\% &  8.46\% &  2.86\% & 73.09\%\\
ter Braak        & 83.59\% & 11.07\% &  5.34\% & 73.38\%\\
Kennedy          & 77.60\% &  1.04\% & 21.35\% & 74.81\%\\
Manly            & 73.31\% & 15.89\% & 10.81\% & 73.38\%\\
Smith            & 89.32\% &  7.81\% &  2.86\% & 72.90\%\\
Huh--Jhun        & 85.81\% &  9.24\% &  4.95\% & 71.62\%\\
Parametric       & 77.47\% & 14.84\% &  7.68\% & 72.73\%\\
\bottomrule
\end{tabular}}
\end{center}
{\footnotesize
When the amount of errors is below the nominal level (here, $\alpha=0.05)$, the test is said to be \emph{conservative}. If above, it is \emph{invalid}.\par}
\label{tab:summary}
\end{table}


\begin{table}[b!]
\caption[Amount of error type \textsc{i} for representative simulation scenarios.]{\emph{(Page \pageref{tab:methods_resultsT})} Proportion of error type \textsc{i} (for $\alpha$ = 0.05), for some representative of the 768 simulation scenarios that did not have signal, using the different permutation methods, and with $G$ as the statistic in the absence of \textsc{eb} (so, equivalent to the $F$ statistic). Confidence intervals (95\%) are shown in parenthesis.}
{\footnotesize
$N$: number of observations;
$\mathbf{x}_1$ and $\mathbf{z}_1$: regressors of interest and of no interest, respectively, being either continuous (\textsc{c}) or discrete (\textsc{d}).
$\rho$: correlation between $\mathbf{x}_1$ and $\mathbf{z}_1$;
\raisebox{-1pt}{\ding{'42}}: model partitioned or not (using the \citet{Beckmann2001} scheme, shown in \ref{sec:partitioning});
$\mathbf{\epsilon}$: distribution of the simulated errors, which can be normal ($\mathcal{N}$), uniform ($\mathcal{U}$), exponential ($\mathcal{E}$) or Weibull ($\mathcal{W}$);
\textsc{ee}: errors treated as exchangeable;
\textsc{ise}: errors treated as independent and symmetric.
The methods are the same shown in Table~\ref{tab:methods}: Draper--Stoneman (D--S), Still--White (S--W), Freedman--Lane (F--L), ter Braak (tB), Kennedy (K), Manly (M), Huh--Jhun (H--J), Smith (S) and parametric (P), the last not using permutations.
\par}
\label{tab:methods_resultsC}
\end{table}


\begin{sidewaystable}
\begin{center}
{\footnotesize
\begin{tabular}{@{}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{2.5mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{}}
\toprule
\multicolumn{8}{c}{Simulation parameters} & \multicolumn{9}{c}{Proportion of error type \textsc{i} (\%)}\\
\cmidrule(r){1-8}\cmidrule(l){9-17}
$N$ & $\mathbf{x}_1$ & $\mathbf{z}_1$ & $\rho$ & \raisebox{-1pt}{\ding{'42}} & $\boldsymbol{\epsilon}$ & \textsc{ee} & \textsc{ise} & D--S & S--W & F--L & tB & K & M & S & H--J & P \\
\midrule
12 & \textsc{c} & \textsc{c} & 0   & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'67} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)}\\
12 & \textsc{c} & \textsc{c} & 0   & \ding{'67} & $\mathcal{U}$ & \ding{'63} & \ding{'63} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 6.9 \scalebox{.7}[1.0]{(5.5--8.6)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 6.9 \scalebox{.7}[1.0]{(5.5--8.6)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)}\\
12 & \textsc{c} & \textsc{c} & 0   & \ding{'67} & $\mathcal{W}$ & \ding{'63} & \ding{'67} & 5.9 \scalebox{.7}[1.0]{(4.6--7.5)} & 6.5 \scalebox{.7}[1.0]{(5.1--8.2)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 6.5 \scalebox{.7}[1.0]{(5.1--8.2)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.9 \scalebox{.7}[1.0]{(4.6--7.5)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 8.3 \scalebox{.7}[1.0]{(6.7--10.2)}\\
12 & \textsc{c} & \textsc{c} & 0   & \ding{'67} & $\mathcal{E}$ & \ding{'63} & \ding{'63} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 6.9 \scalebox{.7}[1.0]{(5.5--8.6)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 6.9 \scalebox{.7}[1.0]{(5.5--8.6)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)}\\
12 & \textsc{c} & \textsc{c} & 0.8 & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'67} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 3.6 \scalebox{.7}[1.0]{(2.6--4.9)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)}\\
12 & \textsc{c} & \textsc{c} & 0.8 & \ding{'67} & $\mathcal{W}$ & \ding{'63} & \ding{'67} & 1.5 \scalebox{.7}[1.0]{(0.9--2.5)} & 1.2 \scalebox{.7}[1.0]{(0.7--2.1)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 6.5 \scalebox{.7}[1.0]{(5.1--8.2)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)} & 8.5 \scalebox{.7}[1.0]{(6.9--10.4)}\\
12 & \textsc{c} & \textsc{c} & 0.8 & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'63} & 5.5 \scalebox{.7}[1.0]{(4.2--7.1)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 7.5 \scalebox{.7}[1.0]{(6.0--9.3)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)}\\
12 & \textsc{c} & \textsc{c} & 0.8 & \ding{'63} & $\mathcal{N}$ & \ding{'63} & \ding{'63} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 7.2 \scalebox{.7}[1.0]{(5.8--9.0)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 7.2 \scalebox{.7}[1.0]{(5.8--9.0)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)}\\
12 & \textsc{c} & \textsc{d} & 0   & \ding{'67} & $\mathcal{W}$ & \ding{'63} & \ding{'67} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 6.8 \scalebox{.7}[1.0]{(5.4--8.5)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 6.8 \scalebox{.7}[1.0]{(5.4--8.5)} & 4.0 \scalebox{.7}[1.0]{(3.0--5.4)} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 3.7 \scalebox{.7}[1.0]{(2.7--5.1)} & 8.9 \scalebox{.7}[1.0]{(7.3--10.8)}\\
12 & \textsc{c} & \textsc{d} & 0   & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'67} & 3.9 \scalebox{.7}[1.0]{(2.9--5.3)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 3.9 \scalebox{.7}[1.0]{(2.9--5.3)} & 4.0 \scalebox{.7}[1.0]{(3.0--5.4)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 3.9 \scalebox{.7}[1.0]{(2.9--5.3)} & 4.2 \scalebox{.7}[1.0]{(3.1--5.6)} & 3.7 \scalebox{.7}[1.0]{(2.7--5.1)}\\
12 & \textsc{c} & \textsc{d} & 0   & \ding{'67} & $\mathcal{W}$ & \ding{'67} & \ding{'63} & 2.9 \scalebox{.7}[1.0]{(2.0--4.1)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 2.6 \scalebox{.7}[1.0]{(1.8--3.8)} & 2.8 \scalebox{.7}[1.0]{(1.9--4.0)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 14.1 \scalebox{.7}[1.0]{(12.1--16.4)} & 2.9 \scalebox{.7}[1.0]{(2.0--4.1)} & 16.4 \scalebox{.7}[1.0]{(14.2--18.8)} & 9.0 \scalebox{.7}[1.0]{(7.4--10.9)}\\
12 & \textsc{d} & \textsc{d} & 0   & \ding{'67} & $\mathcal{W}$ & \ding{'63} & \ding{'67} & 3.2 \scalebox{.7}[1.0]{(2.3--4.5)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 2.2 \scalebox{.7}[1.0]{(1.5--3.3)} & 2.0 \scalebox{.7}[1.0]{(1.3--3.1)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 3.8 \scalebox{.7}[1.0]{(2.8--5.2)} & 3.2 \scalebox{.7}[1.0]{(2.3--4.5)} & 2.6 \scalebox{.7}[1.0]{(1.8--3.8)} & 0.5 \scalebox{.7}[1.0]{(0.2--1.2)}\\
24 & \textsc{c} & \textsc{c} & 0.8 & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'67} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 3.5 \scalebox{.7}[1.0]{(2.5--4.8)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)}\\
24 & \textsc{d} & \textsc{d} & 0   & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'67} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)}\\
24 & \textsc{d} & \textsc{d} & 0   & \ding{'67} & $\mathcal{U}$ & \ding{'63} & \ding{'67} & 6.2 \scalebox{.7}[1.0]{(4.9--7.9)} & 6.6 \scalebox{.7}[1.0]{(5.2--8.3)} & 6.3 \scalebox{.7}[1.0]{(5.0--8.0)} & 5.9 \scalebox{.7}[1.0]{(4.6--7.5)} & 6.6 \scalebox{.7}[1.0]{(5.2--8.3)} & 5.5 \scalebox{.7}[1.0]{(4.2--7.1)} & 6.2 \scalebox{.7}[1.0]{(4.9--7.9)} & 5.9 \scalebox{.7}[1.0]{(4.6--7.5)} & 5.8 \scalebox{.7}[1.0]{(4.5--7.4)}\\
24 & \textsc{d} & \textsc{d} & 0.8 & \ding{'67} & $\mathcal{U}$ & \ding{'63} & \ding{'67} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 1.8 \scalebox{.7}[1.0]{(1.1--2.8)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)}\\
48 & \textsc{c} & \textsc{c} & 0   & \ding{'67} & $\mathcal{N}$ & \ding{'67} & \ding{'63} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 3.8 \scalebox{.7}[1.0]{(2.8--5.2)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 6.0 \scalebox{.7}[1.0]{(4.7--7.6)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)}\\
48 & \textsc{c} & \textsc{c} & 0.8 & \ding{'63} & $\mathcal{U}$ & \ding{'63} & \ding{'67} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)} & 5.6 \scalebox{.7}[1.0]{(4.3--7.2)}\\
48 & \textsc{c} & \textsc{c} & 0.8 & \ding{'63} & $\mathcal{N}$ & \ding{'63} & \ding{'67} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)}\\
48 & \textsc{c} & \textsc{d} & 0   & \ding{'67} & $\mathcal{E}$ & \ding{'67} & \ding{'63} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.5 \scalebox{.7}[1.0]{(4.2--7.1)} & 5.7 \scalebox{.7}[1.0]{(4.4--7.3)} & 9.2 \scalebox{.7}[1.0]{(7.6--11.2)} & 5.4 \scalebox{.7}[1.0]{(4.2--7.0)} & 4.3 \scalebox{.7}[1.0]{(3.2--5.7)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)}\\
48 & \textsc{c} & \textsc{d} & 0.8 & \ding{'67} & $\mathcal{E}$ & \ding{'63} & \ding{'67} & 5.5 \scalebox{.7}[1.0]{(4.2--7.1)} & 0.3 \scalebox{.7}[1.0]{(0.1--0.9)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)}\\
96 & \textsc{c} & \textsc{c} & 0   & \ding{'67} & $\mathcal{N}$ & \ding{'63} & \ding{'63} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 4.6 \scalebox{.7}[1.0]{(3.5--6.1)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.3 \scalebox{.7}[1.0]{(4.1--6.9)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)}\\
96 & \textsc{c} & \textsc{c} & 0.8 & \ding{'67} & $\mathcal{N}$ & \ding{'67} & \ding{'63} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 3.6 \scalebox{.7}[1.0]{(2.6--4.9)} & 5.0 \scalebox{.7}[1.0]{(3.8--6.5)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 4.4 \scalebox{.7}[1.0]{(3.3--5.9)} & 5.1 \scalebox{.7}[1.0]{(3.9--6.6)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)}\\
96 & \textsc{d} & \textsc{c} & 0   & \ding{'67} & $\mathcal{W}$ & \ding{'63} & \ding{'67} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 4.7 \scalebox{.7}[1.0]{(3.6--6.2)} & 4.8 \scalebox{.7}[1.0]{(3.6--6.3)} & 5.2 \scalebox{.7}[1.0]{(4.0--6.8)} & 4.5 \scalebox{.7}[1.0]{(3.4--6.0)} & 4.9 \scalebox{.7}[1.0]{(3.7--6.4)} & 3.9 \scalebox{.7}[1.0]{(2.9--5.3)} & 3.6 \scalebox{.7}[1.0]{(2.6--4.9)}\\
\bottomrule
\multicolumn{17}{l}{\emph{See caption on page \pageref{tab:methods_resultsC}.}}
\end{tabular}}
\end{center}
\label{tab:methods_resultsT}
\end{sidewaystable}

Some of the methods are identical to each other in certain circumstances. If $\mathbf{X}$ and $\mathbf{Z}$ are orthogonal, Draper--Stoneman and Smith are equivalent. Likewise under orthogonality, Still--White produces identical regression coefficients as Freedman--Lane, although the statistic will only be the same if the loss in degrees of freedom due to $\mathbf{Z}$ is taken into account, something not always possible when the data has already been residualised and no information about the original nuisance variables is available. Nonetheless, the two methods remain asymptotically equivalent as the number of observations diverges from the number of nuisance regressors.

\paragraph{Sample size} Increasing the sample size had the effect of approaching the error rate to closer to the nominal level $\alpha=0.05$ for all methods in virtually all parameter configurations. For small samples, most methods were slightly conservative, whereas Still--White and Kennedy were anticonservative and often invalid, particularly if the distributions of the errors were skewed.

\paragraph{Continuous or categorical regressors of interest} For all methods, using continuous or categorical regressors of interest did not produce remarkable differences in the observed proportions of type~\textsc{i} error, except if the distribution of the errors was skewed and sign flipping was used (in violation of assumptions), in which case Manly and Huh--Jhun methods showed erratic control over the amount of errors.

\paragraph{Continuous or categorical nuisance regressors} The presence of continuous or categorical nuisance variables did not substantially interfere with either control over error type~\textsc{i} or power, for any of the methods, except in the presence of correlated regressors.

\paragraph{Degree of non-orthogonality and partitioning} All methods provided relatively adequate control over error type~\textsc{i} in the presence of a correlated nuisance regressor, except Still--White (conservative) and Kennedy (inflated rates). The partitioning scheme mitigated the conservativeness of the former, and the anticonservativeness of the latter.

\paragraph{Distribution of the errors} Different distributions did not substantially improve or worsen error rates when using permutation alone. Still--White and Kennedy tended to fail control over error type~\textsc{i} in virtually all situations. Sign flipping alone, when used with asymmetric distributions (in violation of assumptions), required larger samples to allow approximately exact control over the amount of error type \textsc{i}. In these cases, and with small samples, the methods Draper--Stoneman, Manly and Huh--Jhun tended to display erratic behaviour, with extremes of conservativeness and anticonservativeness depending on the other simulation parameters. The same happened with the parametric method. Freedman--Lane and Smith methods, on the other hand, tended to have a relatively constant and somewhat conservative behaviour in these situations. Permutation combined with sign-flipping generally alleviated these issues where they were observed.

From all the methods, the Freedman--Lane and Smith were those that performed better in most cases, and with their 95\% confidence interval covering the desired error level of 0.05 more often than any of the other methods. The Still--White and Kennedy methods did not generally control the error type~\textsc{i} for most of the simulation parameters, particularly for smaller sample sizes. On the other hand, with a few exceptions, the Freedman--Lane and the Smith methods effectively controlled the error rates in most cases, even with skewed errors and sign-flipping, being, at worst, conservative or only slightly above the nominal level. All methods were, overall, similarly powerful, with only marginal differences among those that were on average valid.

\section{Discussion}

Ideally, criteria to accept or reject a given hypothesis should be sensitive to changes in the parameters of interest (powerful), and insensitive to changes in nuisance factors (robust). As pointed out long ago by \citet{Box1955}, the assumptions on which parametric tests are built are such that the first criterion is generally satisfied, albeit not necessarily the second. Many non-parametric tests, on the other hand, are constructed such that most or all these assumptions are not demanded, satisfying the second criterion, but not necessarily the first. For current applications in neuroimaging, however, this compromise between robustness and power gains new contours and a different balance. First, in neuroimaging it is necessary to address the multiple testing problem, in which one or more tests are applied to each of thousands of points (commonly voxels, vertices or faces) of the image representation of the brain. Parametric methods require the introduction of an even larger set of assumptions to deal with multiple testing. Second, different imaging modalities not necessarily follow the same set of assumptions regarding distributions under the null at each test, neither for the covariance between tests across the brain, so that those that might be acceptably used with one method, may cause others to be invalid. Third, under non-random sampling, as common in case-control studies, the very presence of the features under investigation (such as a disorder) may compromise the assumptions on which parametric tests depend. For all these reasons, parametric methods, despite common use, are more likely to fail as candidates to provide a general statistical framework for the current variety of imaging modalities for research applications, where not only the assumptions may not be met, but also where robustness may be seen as a key factor. Permutation methods are a viable alternative, flexible enough to accommodate several experimental needs. Further to all this, our simulations showed similar and sometimes higher power compared to the parametric approach.

\subsection{Permutation tests}

Permutation tests require very few assumptions about the data and, therefore, can be applied in a wider variety of situations than parametric tests. None of the most common parametric assumptions need to hold for non-parametric tests to be valid. The assumptions that are eschewed include, for instance, the need of normality for the error terms, the need of homoscedasticity and the need of random sampling. With a very basic knowledge of sample properties or of the study design, errors can be treated as exchangeable (\textsc{ee}) and/or independent and symmetric (\textsc{ise}) and inferences that otherwise would not be possible with parametric methods become feasible. Furthermore, permutation tests permit the use of the very same regression and hypothesis testing framework, even with disparate imaging modalities, without the need to verify the validity of parametric assumptions for each of them. The \textsc{ise} can be an alternative to \textsc{ee} when the errors themselves can be considered exchangeable, but the design is not affected by permutations, as for one-sample tests. And if the assumptions for \textsc{ee} and \textsc{ise} are both met, permutation and sign flipping can both be performed to construct the empirical distribution.

The justification for permutation tests has, moreover, more solid foundations than their parametric counterparts. While the validity of parametric tests rely on random sampling, permutation tests rely on the idea of random allocation of experimental units, with no reference to any underlying population. This aspect has a key importance in biomedical research --- including neuroimaging --- where only a small minority of studies effectively use random population sampling. Most experimental studies need to use the subjects that are available in a given area, and who accept to participate (e.g. patients of a hospital or students of an university near where the \textsc{mri} equipment is installed). True random sampling is rarely achieved in real applications because, often and for different reasons, selection criteria are not truly unbiased \citep{Ludbrook1998, Pesarin2010}. Non-parametric methods allow valid inferences to be performed in these scenarios.

\subsection{Pivotal statistics}

In addition, permutation methods have the remarkable feature of allowing the use of non-standard statistics, or for which closed mathematical forms have not been derived, even asymptotically. Statistics that can be used include, for instance, those based on ranks of observations \citep{Brunner2000, Rorden2007}, derived from regression methods other than least squares \citep{Cade1996} or that are robust to outliers \citep{Theil1950, Sen1968}. For imaging applications, statistics that can be considered include the pseudo-$t$ statistic after variance smoothing \citep{Holmes1996}, the mass of connected voxels \citep{Bullmore1999}, threshold-free cluster enhancement (\textsc{tfce}) \citep{Smith2009}, as well as cases in which the distribution of the statistic may lie in a gradient between distributions, each of them with known analytical forms \citep{Winkler2012}. The only requirement, in the context of neuroimaging, is that these statistics retain their distributional properties irrespective to the (unknown) population parameters.

Indeed, a large part of the voluminous literature on statistical tests when the errors cannot be assumed to be homoscedastic is concerned with the identification of the asymptotic distribution of the statistics, its analytical form, and the consequences of experimental scenarios that include unbalancedness and/or small samples. This is true even considering that in parametric settings, the statistics are invariably chosen such that their sampling distribution is independent of underlying and unknown population parameters. Permutation tests render all these issues irrelevant, as the asymptotic properties of the distributions do not need to be ascertained. For imaging, all that is needed is that the distribution remains invariant to unknown population parameters, i.e., the statistic needs to be pivotal. Parameters of the distribution proper do not need to be known, nor the distribution needs to be characterised analytically. The proposed statistic $G$, being a generalisation over various tests that have their niche applications in parametric settings, is appropriate for use with the general linear model and with a permutation framework, for being pivotal and easily implementable using simple matrix operations. Moreover, as the simulations showed, this statistic is not less powerful than the commonly used $F$ statistic.

\subsection{Permutation strategies}

From the different permutation strategies presented in Table~\ref{tab:methods}, the Freedman--Lane and the Smith methods provided the most adequate control of type~\textsc{i} error across the various simulation scenarios. This is in line with the study by \citet{Anderson1999}, who found that the Freedman--Lane method is the most accurate and powerful in various different models. The Smith method was a somewhat positive surprise, not only for the overall very good performance in our simulations, but also because this method had not been extensively evaluated in previous literature, is computationally simple, and has an intuitive appeal.

\citet{Welch1990} commented that the Freedman--Lane procedure would violate the ancillarity principle, as the permutation procedure would destroy the relationship between $\mathbf{X}$ and $\mathbf{Z}$, even if these are orthogonal. Notwithstanding, even with ancillarity violated, this and other methods perform satisfactorily well.

\citet{Freedman1983} described their method as having a ``non-stochastic'' interpretation, and so, that the computed p-value would be a descriptive statistic. On the contrary, we share the same view expressed by \citet{Anderson1999}, that the rationale for the test and the procedure effectively produces a p-value that can be interpreted as a true probability for the underlying model.

Regarding differences between the methods, and even though for this study we did not evaluate the effect of extremely strong signals or of outliers, it is worth commenting that previous research have shown that the Freedman--Lane method is relatively robust to the presence of extreme outliers, whereas the ter Braak tends to become more conservative in these cases \citep{Anderson1999}. The ter Braak method, however, was shown to be more robust to extremely strong signals in the data, situations in which signal may ``leak'' into the permutation distribution with the Freedman--Lane method \citep{Salimi-Khorshidi2011}.

It should be noted that the Still--White method, as implemented for these simulations, used for the regression the model containing only the regressors of interest when computing the statistic. as shown in Table~\ref{tab:methods}. It is done in this way to emulate what probably is its more common use, i.e., rearrange the data that has already been residualised from nuisance, and when the nuisance regressors are no longer available. Had the full model been used when computing the statistic, it is possible that this method might have performed somewhat similarly as Freedman--Lane, specially for larger samples. Moreover, neither the original publication \citep{Still1981}, nor a related method published shortly after \citep{Levin1983}, specify how the degrees of freedom should be treated when computing the statistic in a generic formulation as we present here.

Finally, although non-parametric methods are generally considered less powerful than their parametric counterparts, we found in the simulations performed that most of the permutation methods are not substantially less powerful than the parametric method, and sometimes are even more powerful, even when the assumptions of the latter are met. With the availability of computing power and reliable software implementation, there is almost no reason for not using these permutation methods.

\section{Worked examples}
\label{sec:examples}

The examples below serve to illustrate the permutation aspects discussed in the paper, all with tiny samples, $N=12$ only, so that the design matrices can be shown in their full extent. While permutation tests in general remain valid even with such small samples, these examples are by no means to be understood as a recommendation for sample sizes. There are many reasons why larger samples are more appropriate (see \citet{Button2013} for a recent review), and in what concerns permutations methods, larger samples allow smaller p-values, improve the variance estimates for each \textsc{vg} (which are embodied in the weighting matrix under restricted exchangeability), and allow finer control over the familywise error rate. For each example, the relevant contrasts are also shown.

\newpage
\paragraph{Example 1: Mean effect} Consider a multi-subject \textsc{fmri} study to investigate the \textsc{bold} response associated with a novel experimental task. After the first-level analysis (within subject), maps of contrasts of parameter estimates for each subject are used in a second level analysis. The design matrix for the mean effect is simply a column of ones, and permutations of the data or of the design matrix do not change the model with respect to the regressor of interest. However, by treating the errors as symmetric, instead of permutation, the signs of the ones in the design matrix, or of each datapoint, can be flipped randomly to create the empirical distribution from which inference can be performed. In the presence of nuisance variables, such as handedness, the procedure is performed as in either the Freedman--Lane or Smith methods, replacing the permutation matrix for a sign flipping matrix (Table~\ref{tab:ex_meaneffect}).

\begin{table}[!t]
\caption[Coding for Example 1]{Coding of the design matrix, exchangeability blocks and variance groups for \textbf{Example~1}. Under unrestricted exchangeability, all subjects are assigned to a single block, and with identical variances, all to a single variance group. The regressor $\mathbf{m}_1$ codes for the overall mean, whereas $\mathbf{m}_2$ codes for handedness.}
\begin{center}
{\small
\begin{tabular}{@{}lccrr@{}}
\toprule
\multirow{2}{*}{\vspace*{-1.7mm}Coded data ($\mathbf{Y}$)} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{eb}} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{vg}} & \multicolumn{2}{c}{Model ($\mathbf{M}$)\hspace*{-2mm}}\\
\cmidrule(l){4-5}
& & &  $\mathbf{m}_1$ & $\mathbf{m}_2$\\
\midrule
Subject 1  & 1 & 1 & 1 & $h_{1}$\\
Subject 2  & 1 & 1 & 1 & $h_{2}$\\
Subject 3  & 1 & 1 & 1 & $h_{3}$\\
Subject 4  & 1 & 1 & 1 & $h_{4}$\\
Subject 5  & 1 & 1 & 1 & $h_{5}$\\
Subject 6  & 1 & 1 & 1 & $h_{6}$\\
Subject 7  & 1 & 1 & 1 & $h_{7}$\\
Subject 8  & 1 & 1 & 1 & $h_{8}$\\
Subject 9  & 1 & 1 & 1 & $h_{9}$\\
Subject 10 & 1 & 1 & 1 & $h_{10}$\\
Subject 11 & 1 & 1 & 1 & $h_{11}$\\
Subject 12 & 1 & 1 & 1 & $h_{12}$\\
\midrule
Contrast 1 ($\mathbf{C}'_1$) & & & $+1$ & 0\\
Contrast 2 ($\mathbf{C}'_2$) & & & $-1$ & 0\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ex_meaneffect}
\end{table}

\newpage
\paragraph{Example 2: Multiple regression} Consider the analysis of a study that compares patients and controls with respect to brain cortical thickness, and that recruiting process ensured that all selected subjects are exchangeable. Elder subjects may, however, have thinner cortices, regardless of the diagnosis so, without considering the possibility of interaction. To control for the confounding effect of age, it is included in the design as a nuisance regressor. Sex is also included. The permutation strategy follows the Freedman--Lane or Smith methods, with the residuals of the reduced model being permuted under unrestricted exchangeability (Table~\ref{tab:ex_multipleregression}).

\begin{table}[!t]
\caption[Coding for Example 2]{Coding for \textbf{Example~2}. Under unrestricted exchangeability, all subjects are assigned to a single block. The regressors $\mathbf{m}_1$ and $\mathbf{m}_2$ code for the experimental groups, $\mathbf{m}_3$ and $\mathbf{m}_4$ for age and sex.}
\begin{center}
{\small
\begin{tabular}{@{}lccrrrr@{}}
\toprule
\multirow{2}{*}{\vspace*{-1.7mm}Coded data ($\mathbf{Y}$)} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{eb}} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{vg}} & \multicolumn{4}{c}{Model ($\mathbf{M}$)\hspace*{-3mm}}\\
\cmidrule(l){4-7}
& & & $\mathbf{m}_1$ & $\mathbf{m}_2$ & $\mathbf{m}_3$ & $\mathbf{m}_4$\\
\midrule
Subject 1   & 1 & 1 & 1 & 0 & $a_{1}$  & $s_{1}$\\
Subject 2   & 1 & 1 & 1 & 0 & $a_{2}$  & $s_{2}$\\
Subject 3   & 1 & 1 & 1 & 0 & $a_{3}$  & $s_{3}$\\
Subject 4   & 1 & 1 & 1 & 0 & $a_{4}$  & $s_{4}$\\
Subject 5   & 1 & 1 & 1 & 0 & $a_{5}$  & $s_{5}$\\
Subject 6   & 1 & 1 & 1 & 0 & $a_{6}$  & $s_{6}$\\
Subject 7   & 1 & 1 & 0 & 1 & $a_{7}$  & $s_{7}$\\
Subject 8   & 1 & 1 & 0 & 1 & $a_{8}$  & $s_{8}$\\
Subject 9   & 1 & 1 & 0 & 1 & $a_{9}$  & $s_{9}$\\
Subject 10  & 1 & 1 & 0 & 1 & $a_{10}$ & $s_{10}$\\
Subject 11  & 1 & 1 & 0 & 1 & $a_{11}$ & $s_{11}$\\
Subject 12  & 1 & 1 & 0 & 1 & $a_{12}$ & $s_{12}$\\
\midrule
Contrast 1 ($\mathbf{C}'_1$) & & & $+1$ & $-1$ & 0 & 0\\
Contrast 2 ($\mathbf{C}'_2$) & & & $-1$ & $+1$ & 0 & 0\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ex_multipleregression}
\end{table}

\newpage
\paragraph{Example 3: Paired $t$-test} Consider a study to investigate the effect of the use of a certain analgesic in the magnitude of the \textsc{bold} response associated with painful stimulation. In this example, the response after the treatment is compared with the response before the treatment, i.e., each subject is their own control. The experimental design is the ``paired $t$-test''. One \textsc{eb} is defined per subject, as the observatios are not exchangeable across subjects, and as the variance can be assumed to be homogeneous across all observations, only one \textsc{vg} is defined encompassing all observations (Table~\ref{tab:ex_pairedttest}).

\begin{table}[!t]
\caption[Coding for Example 3]{Coding of the design matrix exchangebility blocks and variance groups for \textbf{Example~3}. Observations are exchangeable only within subject, and variance can be estimated considering all observations as a single group. The regressor $\mathbf{m}_1$ codes for treatment, whereas $\mathbf{m}_2$ to $\mathbf{m}_7$ code for subject-specific mean.}
\begin{center}
{\small
\begin{tabular}{@{}lccrrrrrrr@{}}
\toprule
\multirow{2}{*}{\vspace*{-1.7mm}Coded data ($\mathbf{Y}$)} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{eb}} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{vg}} & \multicolumn{7}{c}{Model ($\mathbf{M}$)\hspace*{-3mm}}\\
\cmidrule(l){4-10}
& & & $\mathbf{m}_1$ & $\mathbf{m}_2$ & $\mathbf{m}_3$ & $\mathbf{m}_4$& $\mathbf{m}_5$ & $\mathbf{m}_6$ & $\mathbf{m}_7$\\
\midrule
Subj.\ 1, obs.\ 1  & 1 & 1 & $+1$ & 1 & 0 & 0 & 0 & 0 & 0\\
Subj.\ 2, obs.\ 1  & 2 & 1 & $+1$ & 0 & 1 & 0 & 0 & 0 & 0\\
Subj.\ 3, obs.\ 1  & 3 & 1 & $+1$ & 0 & 0 & 1 & 0 & 0 & 0\\
Subj.\ 4, obs.\ 1  & 4 & 1 & $+1$ & 0 & 0 & 0 & 1 & 0 & 0\\
Subj.\ 5, obs.\ 1  & 5 & 1 & $+1$ & 0 & 0 & 0 & 0 & 1 & 0\\
Subj.\ 6, obs.\ 1  & 6 & 1 & $+1$ & 0 & 0 & 0 & 0 & 0 & 1\\
Subj.\ 1, obs.\ 2  & 1 & 1 & $-1$ & 1 & 0 & 0 & 0 & 0 & 0\\
Subj.\ 2, obs.\ 2  & 2 & 1 & $-1$ & 0 & 1 & 0 & 0 & 0 & 0\\ 
Subj.\ 3, obs.\ 2  & 3 & 1 & $-1$ & 0 & 0 & 1 & 0 & 0 & 0\\ 
Subj.\ 4, obs.\ 2  & 4 & 1 & $-1$ & 0 & 0 & 0 & 1 & 0 & 0\\
Subj.\ 5, obs.\ 2  & 5 & 1 & $-1$ & 0 & 0 & 0 & 0 & 1 & 0\\
Subj.\ 6, obs.\ 2  & 6 & 1 & $-1$ & 0 & 0 & 0 & 0 & 0 & 1\\
\midrule
Contrast 1 ($\mathbf{C}'_1$) & & & $+1$ & 0 & 0 & 0 & 0 & 0 & 0\\
Contrast 2 ($\mathbf{C}'_2$) & & & $-1$ & 0 & 0 & 0 & 0 & 0 & 0\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ex_pairedttest}
\end{table}

\newpage
\paragraph{Example 4: Unequal group variances}  Consider a study using \textsc{fmri} to compare whether the \textsc{bold} response associated with a certain cognitive task would differ among subjects with autistic spectrum disorder (\textsc{asd}) and control subjects, while taking into account differences in age and sex. In this hypothetical example, the cognitive task is known to produce more erratic signal changes in the patient group than in controls. Therefore, variances cannot be assumed to be homogeneous with respect to the group assignment of subjects. This is an example of the classical Behrens--Fisher problem. To accommodate heteroscedasticity, two permutation blocks are defined according to the group of subjects. Under the assumption of independent and symmetric errors, the problem is solved by means of random sign-flipping \citep{Pesarin1995}, using the well known Welch's $v$ statistic, a particular case of the statistic $G$ shown in Equation~\ref{eqn:gstat} (Table~\ref{tab:ex_behrensfisher}).

\begin{table}[!t]
\caption[Coding for Example 4]{Coding of the design matrix and exchangebility blocks for \textbf{Example~4}. As the group variances cannot be assumed to be the same, each group constitutes an EB and VG; sign flippings happen within block. The regressors $\mathbf{m}_1$ and $\mathbf{m}_2$ code for the experimental groups, $\mathbf{m}_3$ and $\mathbf{m}_4$ for age and sex.}
\begin{center}
{\small
\begin{tabular}{@{}lccrrrr@{}}
\toprule
\multirow{2}{*}{\vspace*{-1.7mm}Coded data ($\mathbf{Y}$)} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{eb}} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{vg}} & \multicolumn{4}{c}{Model ($\mathbf{M}$)\hspace*{-3mm}}\\
\cmidrule(l){4-7}
& & & $\mathbf{m}_1$ & $\mathbf{m}_2$ & $\mathbf{m}_3$ & $\mathbf{m}_4$\\
\midrule
Subject 1   & 1 & 1 & 1 & 0 & $a_{1}$  & $s_{1}$\\
Subject 2   & 1 & 1 & 1 & 0 & $a_{2}$  & $s_{2}$\\
Subject 3   & 1 & 1 & 1 & 0 & $a_{3}$  & $s_{3}$\\
Subject 4   & 1 & 1 & 1 & 0 & $a_{4}$  & $s_{4}$\\
Subject 5   & 1 & 1 & 1 & 0 & $a_{5}$  & $s_{5}$\\
Subject 6   & 1 & 1 & 1 & 0 & $a_{6}$  & $s_{6}$\\
Subject 7   & 2 & 2 & 0 & 1 & $a_{7}$  & $s_{7}$\\
Subject 8   & 2 & 2 & 0 & 1 & $a_{8}$  & $s_{8}$\\
Subject 9   & 2 & 2 & 0 & 1 & $a_{9}$  & $s_{9}$\\
Subject 10  & 2 & 2 & 0 & 1 & $a_{10}$ & $s_{10}$\\
Subject 11  & 2 & 2 & 0 & 1 & $a_{11}$ & $s_{11}$\\
Subject 12  & 2 & 2 & 0 & 1 & $a_{12}$ & $s_{12}$\\
\midrule
Contrast 1 ($\mathbf{C}'_1$) & & & $+1$ & $-1$ & 0 & 0\\
Contrast 2 ($\mathbf{C}'_2$) & & & $-1$ & $+1$ & 0 & 0\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ex_behrensfisher}
\end{table}

\newpage
\paragraph{Example 5: Variance as a confound} Consider a study using \textsc{fmri} to compare whether a given medication would modify the \textsc{bold} response associated with a certain attention task. The subjects are allocated in two groups, one receiving the drug, the other not. In this hypothetical example, the task is known to produce very robust and, on average, similar responses for male and female subjects, although it is also known that males tend to display more erratic signal changes, either very strong or very weak. Therefore, variances cannot be assumed to be homogeneous with respect to the sex of the subjects. To accommodate heteroscedasticity, two permutation blocks are defined according to sex, and each permutation matrix is constructed such that permutations only happen within each of these blocks (Table~\ref{tab:ex_variance}).

\begin{table}[!t]
\caption[Coding for Example 5]{Coding for \textbf{Example~5}. The different variances restrict exchangeability for within same sex only, and two exchangebility blocks are defined, for shuffling within block. The regressors $\mathbf{m}_1$ and $\mathbf{m}_2$ code for group (patients and controls), whereas $\mathbf{m}_3$ codes for sex.}
\begin{center}
{\small
\begin{tabular}{@{}lccrrr@{}}
\toprule
\multirow{2}{*}{\vspace*{-1.7mm}Coded data ($\mathbf{Y}$)} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{eb}} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{vg}} & \multicolumn{3}{c}{Model ($\mathbf{M}$)\hspace*{-3mm}}\\
\cmidrule(l){4-6}
& & & $\mathbf{m}_1$ & $\mathbf{m}_2$ & $\mathbf{m}_3$\\
\midrule
Subject 1  & 1 & 1 & 1 & 0 & $1$\\
Subject 2  & 1 & 1 & 1 & 0 & $1$\\
Subject 3  & 1 & 1 & 1 & 0 & $1$\\
Subject 4  & 2 & 2 & 1 & 0 & $-1$\\
Subject 5  & 2 & 2 & 1 & 0 & $-1$\\
Subject 6  & 2 & 2 & 1 & 0 & $-1$\\
Subject 7  & 1 & 1 & 0 & 1 & $1$\\
Subject 8  & 1 & 1 & 0 & 1 & $1$\\
Subject 9  & 1 & 1 & 0 & 1 & $1$\\
Subject 10 & 2 & 2 & 0 & 1 & $-1$\\
Subject 11 & 2 & 2 & 0 & 1 & $-1$\\
Subject 12 & 2 & 2 & 0 & 1 & $-1$\\
\midrule
Contrast 1 ($\mathbf{C}'_1$) & & & $1$ & $-1$ & 0\\
Contrast 2 ($\mathbf{C}'_2$) & & & $-1$ & $1$ & 0\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ex_variance}
\end{table}

\newpage
\paragraph{Example 6: Longitudinal study} Consider a study to evaluate whether fractional anisoptropy (\textsc{fa}) would mature differently between boys and girls during middle childhood. Each child recruited to the study is examined three times, at the ages of 9, 10 and 11 years, and none of them are related in any known way. Permutation of observations within child cannot be considered, as the null hypothesis is not the one that \textsc{fa} itself would be zero, but instead, that there would be no changes in the value of \textsc{fa} along the three yearly observations. The permutations must, therefore, always keep in the same order the three observations. Blocks are defined as one per subject, each encompassing all the three observations, and permutation of each block as a whole is performed. If the variances cannot be assumed to be equal along time, one variance group can be defined per time point, otherwise all are assigned to the same \textsc{vg}. If there are nuisance variables to be considered, these can be included in the model and the procedure is performed using the same Freedman--Lane or Smith strategies (Table~\ref{tab:ex_longitudinal}).

\begin{table}[!t]
\caption[Coding for Example 6]{Coding of the design matrix, exchangebility blocks and variance groups for \textbf{Example~6}. Shufflings happen for the blocks as a whole, and variances are not assumed to be the same across all timepoints.}
\begin{center}
{\small
\begin{tabular}{@{}lccrrrrrr@{}}
\toprule
\multirow{2}{*}{\vspace*{-1.7mm}Coded data ($\mathbf{Y}$)} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{eb}} & \multirow{2}{*}{\vspace*{-1.7mm}\textsc{vg}} & \multicolumn{6}{c}{Model ($\mathbf{M}$)\hspace*{-6mm}}\\
\cmidrule(l){4-9}
& & & $\mathbf{m}_1$ & $\mathbf{m}_2$ & $\mathbf{m}_3$ & $\mathbf{m}_4$ & $\mathbf{m}_5$ & $\mathbf{m}_6$\\
\midrule
Subject 1, Timepoint 1 & 1 & 1 & $a_{11}$ & 0 & 1 & 0 & 0 & 0\\
Subject 1, Timepoint 2 & 1 & 2 & $a_{12}$ & 0 & 1 & 0 & 0 & 0\\
Subject 1, Timepoint 3 & 1 & 3 & $a_{13}$ & 0 & 1 & 0 & 0 & 0\\
Subject 2, Timepoint 1 & 2 & 1 & $a_{21}$ & 0 & 0 & 1 & 0 & 0\\
Subject 2, Timepoint 2 & 2 & 2 & $a_{22}$ & 0 & 0 & 1 & 0 & 0\\
Subject 2, Timepoint 3 & 2 & 3 & $a_{23}$ & 0 & 0 & 1 & 0 & 0\\
Subject 3, Timepoint 1 & 3 & 1 & 0 & $a_{31}$ & 0 & 0 & 1 & 0\\
Subject 3, Timepoint 2 & 3 & 2 & 0 & $a_{32}$ & 0 & 0 & 1 & 0\\
Subject 3, Timepoint 3 & 3 & 3 & 0 & $a_{33}$ & 0 & 0 & 1 & 0\\
Subject 4, Timepoint 1 & 4 & 1 & 0 & $a_{41}$ & 0 & 0 & 0 & 1\\
Subject 4, Timepoint 2 & 4 & 2 & 0 & $a_{42}$ & 0 & 0 & 0 & 1\\
Subject 4, Timepoint 3 & 4 & 3 & 0 & $a_{43}$ & 0 & 0 & 0 & 1\\
\midrule
Contrast 1 ($\mathbf{C}'_1$) & & & $1$ & $-1$ & 0 & 0 & 0 & 0\\
Contrast 2 ($\mathbf{C}'_2$) & & & $-1$ & $1$ & 0 & 0 & 0 & 0\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ex_longitudinal}
\end{table}
